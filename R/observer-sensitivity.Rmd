---
output: pdf_document
---

```{r loadPackages, echo=FALSE, message=FALSE, warning=FALSE}
## MCMC parameters
n_chains <- 4
n_adapt <- 5000
n_burn <- 5000
n_iter <- 10000

## Set seed and load libraries/functions
set.seed(11)
library(knitr)
library(sp)
library(raster)
library(Gmisc)
library(loo)
library(runjags)
library(myFunctions)
library(mvtnorm)
library(maps)
library(fields)
library(ggplot2)
library(snowfall)
library(parallel)
library(rlecuyer)
library(ggmcmc)
library(coda)
library(doMC); registerDoMC()
library(rjags); load.module("glm"); load.module("lecuyer")
library(random)
library(xtable)
library(geoR)
library(geosphere)

library(fortFunctions)

image.scale <- function(z, zlim, col = heat.colors(12),
                        breaks, horiz=TRUE, ylim=NULL, xlim=NULL, ...){
  if(!missing(breaks)){
    if(length(breaks) != (length(col)+1)){
      stop("must have one more break than colour")
      }
  }
  if(missing(breaks) & !missing(zlim)){
    breaks <- seq(zlim[1], zlim[2], length.out=(length(col)+1)) 
  }
  if(missing(breaks) & missing(zlim)){
    zlim <- range(z, na.rm=TRUE)
    zlim[2] <- zlim[2]+c(zlim[2]-zlim[1])*(1E-3)#adds a bit to the range in both directions
    zlim[1] <- zlim[1]-c(zlim[2]-zlim[1])*(1E-3)
    breaks <- seq(zlim[1], zlim[2], length.out=(length(col)+1))
  }
  poly <- vector(mode="list", length(col))
  for(i in seq(poly)){
    poly[[i]] <- c(breaks[i], breaks[i+1], breaks[i+1], breaks[i])
  }
  xaxt <- ifelse(horiz, "s", "n")
  yaxt <- ifelse(horiz, "n", "s")
  if(horiz){YLIM<-c(0,1); XLIM<-range(breaks)}
  if(!horiz){YLIM<-range(breaks); XLIM<-c(0,1)}
  if(missing(xlim)) xlim=XLIM
  if(missing(ylim)) ylim=YLIM
  par(mar=c(0.5, 2.5, 1.75, 0.5))
  plot(1,1,t="n",ylim=ylim, xlim=xlim, xaxt=xaxt, yaxt=yaxt, xaxs="i", 
       yaxs="i", las=2, cex.axis=1.75, ...)  
  for(i in seq(poly)){
    if(horiz){
      polygon(poly[[i]], c(0,0,1,1), col=col[i], border=NA)
    }
    if(!horiz){
      polygon(c(0,0,1,1), poly[[i]], col=col[i], border=NA)
    }
  }
}

# make sure directories exist
if (!exists("./models-sens")) {
  dir.create("./models-sens")
}

if (!exists("./sim-fit")) {
  dir.create("./sim-fit")
}

if (!exists("./observer-fit")) {
  dir.create("./observer-fit")
}

if (!exists("./outlier-fit")) {
  dir.create("./outlier-fit")
}

# if(!exists("./diagnostics")) {
#   dir.create("./diagnostics")
# }
# 
# if(!exists("./diagnostics/sim")) {
#   dir.create("./diagnostics/sim")
# }
# 
# if(!exists("./diagnostics/observer")) {
#   dir.create("./diagnostics/observer")
# }
# 
# if(!exists("./diagnostics/outlier-removed")) {
#   dir.create("./diagnostics/outlier-removed")
# }
```

## Simulate Some data

```{r simulateData, echo=FALSE, cache=TRUE, cache.lazy=FALSE, eval=TRUE}
## Simulate data
m <- 1000 # number of spatial locations
locs <- seq(0, 1, , m) # spatial coordinate
W <- cbind(rep(1, m), locs,
           cos(2 * pi * locs), 
           cos(10 * pi * locs),
           cos(4 * pi * locs),
           sin(2 * pi * locs), 
           sin(10 * pi * locs),
           sin(4 * pi * locs)
)
reps <- 50 #150 # number of spatial fields
beta <- matrix(c(rnorm(reps, 0, 0.1), rnorm(reps, 2, 0.1), 
                 seq(-0.25, 0.25, length=reps), 
                 rep(seq(-0.25, 0.25, length=5), 10), 
                 rep(seq(-0.25, 0.25, length=25), 2),
                 seq(-0.25, 0.25, length=reps), 
                 rep(seq(-0.25, 0.25, length=5), 10), 
                 rep(seq(-0.25, 0.25, length=25), 2)),
               dim(W)[2], reps,
               byrow = TRUE) # beta
s2_s <- 1
phi <- 0.25
s2_hist <- 1.5
s2_obs <- 0.75
nu <- 10
samp_size <- 50:200#3:60

field <- makeSpatialFieldT(reps, W, beta, locs, c(s2_s, phi, nu), 
                           method = 'exponential', s2_hist, s2_obs, samp_size)

Y_list <- field$Y_list[1:(reps / 2)] 
H_list <- field$H_list[1:(reps / 2)] 
HMinus_list <- field$HMinus_list[1:(reps / 2)] 
Z_list_noiseless <- field$Z_list[1:(reps / 2)]
Z_list_pca <- field$Z_list[(reps / 2 + 1):reps]
Z_list_noisy <- field$Z_list_observed[(reps / 2 + 1):reps] 
## True spatial field
Z_hist <- matrix(unlist(Z_list_noiseless), ncol = reps / 2, byrow = FALSE)
Z_hist_noisy <- matrix(unlist(field$Z_list_observed[1:(reps / 2)]), 
                       ncol = reps / 2, byrow = FALSE)
X <- matrix(unlist(Z_list_noisy), ncol = reps / 2, byrow = FALSE)
```


```{r processData, echo=FALSE}
## Process the simulated data into a form usable for the model
N <- dim(X)[1]
t <- length(Y_list)
d <- dim(X)[2]
p <- 24
nobs <- rep(0, t)
mu <- rep(0, t)
Y_center <- Y_list
for(i in 1:t){
  nobs[i] <- length(Y_list[[i]])
  mu[i] <- mean(Y_center[[i]])
  Y_center[[i]] <- Y_list[[i]] - mu[i]
}

Y_sparse <- unlist(Y_center)
H_sparse <- unlist(H_list)
tt <- rep(1:t, nobs)
n_thin <- 10
D_array <- array(0, dim=c(t, max(nobs), max(nobs)))
I_t <- array(0, dim=c(t, max(nobs), max(nobs)))
for(i in 1:t){
  nH <- length(H_list[[i]])
  locs_mat <- as.matrix(locs[H_list[[i]]])
  D_array[i, 1:nH, 1:nH] <- makeDistARMA(locs_mat, locs_mat)
  I_t[i, 1:nH, 1:nH] <- diag(nH)
}

pos_idx <- rep(1, t+1)
for(i in 2:(t+1)){
  pos_idx[i] <- pos_idx[i-1] + nobs[i-1];
}
X_PCA <- makePCA(X)
X_PPCA <- makePPCA(X, p)

```

## Plot Simulated data

```{r simPlot, echo=FALSE, include=FALSE, fig.height=6, fig.width=12, eval=TRUE}
library(RColorBrewer)
c25 <- c("dodgerblue2","#E31A1C", # red
         "green4",
         "#6A3D9A", # purple
         "#FF7F00", # orange
         "black","gold1",
         "skyblue2","#FB9A99", # lt pink
         "palegreen2",
         "#CAB2D6", # lt purple
         "#FDBF6F", # lt orange
         "gray70", "khaki2",
         "maroon","orchid1","deeppink1","blue1","steelblue4",
         "darkturquoise","green1","yellow4","yellow3",
         "darkorange4","brown")
layout(matrix(1:4, 2, 2, byrow=TRUE))
par(mar=c(4, 4, 0.5, 0.5) + 0.1, oma=c(0, 0, 0, 0) + 0.1)

# par(mar=c(0.5, 0.5, 1.75, 0.5) + 0.1, oma=c(0, 0, 0.25, 2.5) + 0.1)
matplot(locs, matrix(unlist(Z_list_noisy), m, reps/2), type='n',
        col=adjustcolor("black", alpha.f=0.1),
        ylim =c(min(unlist(Z_list_noisy)), max(unlist(Z_list_noisy))),
        main = '', xlab="", xaxt="n", ylab="Simulated Temperature", las=2)
for(i in 1:(reps/2)) {
  idx <- order(locs[H_list[[i]]])
  matplot(locs[H_list[[i]]][idx], Z_list_noisy[[i]][H_list[[i]]][idx], type='p',
          col=adjustcolor(c25[i], alpha.f=0.25), add=TRUE, pch=16)
}

mtext("(a)",side=3,line=-1.9,adj=c(0.1),cex=1,col="black")

# par(mar=c(1, 0.5, 0.5, 3) + 0.1)
matplot(locs, matrix(unlist(Z_list_pca), m, reps/2), type='n',
        col=adjustcolor("black", alpha.f=0.1), main = '',
        ylim =c(min(unlist(Z_list_noisy)), max(unlist(Z_list_noisy))),
        xlab="", xaxt="n", yaxt="n", ylab="")
for (i in 1:(reps/2)) {
  matplot(locs, Z_list_pca[[i]], type='l', add=TRUE,
          col=adjustcolor(c25[i], alpha.f=0.25),
          xlab="", xaxt="n", yaxt="n", ylab="")
}
mtext("(b)",side=3,line=-1.9,adj=c(0.1),cex=1,col="black")

noisy_plot <- makePCA(matrix(unlist(Z_list_noisy), ncol = reps / 2, 
                             byrow = FALSE))$X_pca[, 1:4]
# noisy_plot[, 1] <- noisy_plot[, 1]

# par(mar=c(1, 0.5, 0.5, 3) + 0.1)
matplot(locs, -noisy_plot, type='l', las=2, xaxt='n',
        ylab="Principal Components",  xlab = "Location", 
        main="", ylim =c(min(noisy_plot), max(noisy_plot)))
axis(1)
mtext("(c)",side=3,line=-1.9,adj=c(0.1),cex=1,col="black")

# par(mar=c(1, 0.5, 0.5, 3) + 0.1)
matplot(locs, makePCA(matrix(unlist(field$Z_list[(reps / 2 + 1):reps]), 
                             ncol = reps / 2, byrow = FALSE))$X_pca[, 1:4], 
        type='l', yaxt="n", ylab="", xlab="Location",
        main = "", ylim =c(min(noisy_plot), max(noisy_plot)))
mtext("(d)",side=3,line=-1.9,adj=c(0.1),cex=1,col="black")
```

\begin{figure}
\centering\includegraphics[width=0.95\linewidth]{observer-sensitivity_files/figure-latex/simPlot-1.pdf}
\caption{Simulation study showing observed noisy data (a), latent climate process (b), noisy principal components (c), and observed principal components (d).}
\label{fig:simPlot}
\end{figure}

## Define the candidate models

```{r PCASSVS, echo=FALSE, eval=TRUE}
cat("model{
  # hierarchically pooled data model sqrt(precision)
  for(i in 1:t){
    s_inv[i] ~ dlnorm(mu_s_inv, tau2_s)                # pooling prior for data model sqrt(precision)
  }
  mu_s_inv ~ dnorm(0, 1 / 1)                         # pooled mean for data model sqrt(precision)
  tau_s ~ dunif(0, 1)                                # pooled sqrt(precision) for data model sqrt(precision)
  # mu_s_inv ~ dnorm(0, 1 / 100)                         # pooled mean for data model sqrt(precision)
  # tau_s ~ dunif(0, 100)                                # pooled sqrt(precision) for data model sqrt(precision)
  tau2_s <- pow(tau_s, 2)                              # pooled precision for data model sqrt(precision)
  
  # hierarchically pooled ssvs priors
  for(i in 1:t){
    tau_beta[i] ~ dlnorm(mu_tau_beta, tau2_tau_beta)   # regression coefficient sqrt(precision)
    tau2_beta[i] <- pow(tau_beta[i], 2)                # regression coefficient precision
    precision_beta[1, i] <- tau2_beta[i]               # regression coefficient effectively zero
    precision_beta[2, i] <- tau2_beta[i] / 1000        # nonzero coefficient
  }
  mu_tau_beta ~ dnorm(0, 1 / 1)                      # pooled mean for regression coefficient sqrt(precision)
  tau_tau_beta ~ dunif(0, 1)                         # pooled sqrt(precision) for regression coefficient sqrt(precision)
  # mu_tau_beta ~ dnorm(0, 1 / 100)                      # pooled mean for regression coefficient sqrt(precision)
  # tau_tau_beta ~ dunif(0, 100)                         # pooled sqrt(precision) for regression coefficient sqrt(precision)
  tau2_tau_beta <- pow(tau_tau_beta, 2)                # pooled precision for  regression coefficient sqrt(precision)
  p_ind[1] <- 1/2                                      # prior probability of nonzero coefficient
  p_ind[2] <- 1 - p_ind[1]                             # prior probability effectively zero coefficient

  for (j in 1:t){
    for(i in 1:p){
      indA[i, j] ~ dcat(p_ind[])                       # sample indicator for nonzero coefficient, returns 1 or 2 
      gamma[i, j] <- indA[i, j] - 1                    # transform indicator from 0-1 to 1-2 for indexing, returns 0 or 1
      beta[i, j] ~ dnorm(0, precision_beta[indA[i, j], j] / D[i])  # ssvs prior for regression coefficients
    }
  }
  
  for (i in 1:N_total){
    mu[i] <- X[H[i], ] %*% beta[, tt[i]]              # data model mean for sparse data format
                                                      # X <- PCA rotation matrix
                                                      # H <- observation indicator
    s2_y_inv[i] <- pow(s_inv[tt[i]], 2)               # data model precision for sparse data format
  }

  # likelihood
  for (i in 1:N_total){
    Y[i] ~ dnorm(mu[i], s2_y_inv[i])                 # data model for sparse data format
  }
}
", file="./models-sens/PCASSVS.txt")
```

```{r PCALasso, echo=FALSE, eval=TRUE}
cat("model{
  # hierarchically pooled data model sqrt(precision)
  for(i in 1:t){
    s_inv[i] ~ dlnorm(mu_s_inv, tau2_s)                # pooling prior for data model sqrt(precision)
    s2[i] <- pow(s_inv[i], -2)                         # data model precision
  }
  mu_s_inv ~ dnorm(0, 1 / 1)                         # pooled mean for data model sqrt(precision)
  tau_s ~ dunif(0, 1)                                # pooled sqrt(precision) for data model sqrt(precision)
  # mu_s_inv ~ dnorm(0, 1 / 100)                         # pooled mean for data model sqrt(precision)
  # tau_s ~ dunif(0, 100)                                # pooled sqrt(precision) for data model sqrt(precision)
  tau2_s <- pow(tau_s, 2)                              # pooled precision for data model sqrt(precision)
  
  for (i in 1:N_total){
    mu_y[i] <- X[H[i], ] %*% beta[, tt[i]]            # data model mean for sparse data format
                                                      # X <- PCA rotation matrix 
                                                      # H <- observation indicator
  }

  # lasso priors
  for(j in 1:t){  
    lambda2[j] ~ dgamma(alpha_lambda2, beta_lambda2)  # hierarchically pooled lasso shrinkage parameter
    for(i in 1:p){
      gamma2[i, j] ~ dexp(lambda2[j] / 2)             # lasso scale mixture parameter
      beta[i, j] ~ dnorm(0, 1 / (s2[j] * gamma2[i, j] * D[i]))  # lasso prior model for PCA 
                                                      # D is a vector of singular values from PCA
    }
  }
  mu_lambda2 ~ dlnorm(0, 1 / 1)                       # hierarchical mean of lambda2 gamma distribution
  s_lambda2 ~ dunif(0, 1)                            # hierarchical standard deviation of lambda2 gamma distribution
  # mu_lambda2 ~ dlnorm(0, 1 / 100)                       # hierarchical mean of lambda2 gamma distribution
  # s_lambda2 ~ dunif(0, 100)                            # hierarchical standard deviation of lambda2 gamma distribution
  alpha_lambda2 <- pow(mu_lambda2, 2) / pow(s_lambda2, 2) # reparameterization of gamma distribution
  beta_lambda2 <- mu_lambda2 / pow(s_lambda2, 2)       # reparameterization of gamma distribution

  # likelihood
  for (i in 1:N_total){
      Y[i] ~ dnorm(mu_y[i], 1 / s2[tt[i]])           # data model for sparse data format
  }
}
", file="./models-sens/PCALasso.txt")
```

```{r tPCASSVS, echo=FALSE, eval=TRUE}
cat("model{
  # hierarchically pooled data model sqrt(precision)
  for(i in 1:t){
    s_inv[i] ~ dlnorm(mu_s_inv, tau2_s)                # pooling prior for data model sqrt(precision)
    s2[i] <- pow(s_inv[i], -2)                         # data model precision
  }
  mu_s_inv ~ dnorm(0, 1 / 1)                         # pooled mean for data model sqrt(precision)
  tau_s ~ dunif(0, 1)                                # pooled sqrt(precision) for data model sqrt(precision)
  # mu_s_inv ~ dnorm(0, 1 / 100)                         # pooled mean for data model sqrt(precision)
  # tau_s ~ dunif(0, 100)                                # pooled sqrt(precision) for data model sqrt(precision)
  tau2_s <- pow(tau_s, 2)                              # pooled precision for data model sqrt(precision)
  
  for (i in 1:N_total){
    mu_y[i] <- X[H[i], ] %*% beta[, tt[i]]             # data model mean for sparse data format
                                                       # X <- PCA rotation matrix
                                                       # H <- observation indicator
    v_inv[i] ~ dchisq(nu[tt[i]])                       # scale mixture for t data model variance
    v[i] <- (nu[tt[i]] * s2[tt[i]]) / v_inv[i]         # transformation to scaled inv-Chi squared variance
  }

  # hierarchically pooled ssvs priors
  for(i in 1:t){
    tau_beta[i] ~ dlnorm(mu_tau_beta, tau2_tau_beta)   # regression coefficient sqrt(precision)
    tau2_beta[i] <- pow(tau_beta[i], 2)                # regression coefficient precision
    precision_beta[1, i] <- tau2_beta[i]               # regression coefficient effectively zero
    precision_beta[2, i] <- tau2_beta[i] / 1000        # nonzero coefficient
  }
  mu_tau_beta ~ dnorm(0, 1 / 1)                           # pooled mean for regression coefficient sqrt(precision)
  tau_tau_beta ~ dunif(0, 1)                         # pooled sqrt(precision) for regression coefficient sqrt(precision)
  # mu_tau_beta ~ dnorm(0, 1 / 100)                           # pooled mean for regression coefficient sqrt(precision)
  # tau_tau_beta ~ dunif(0, 100)                         # pooled sqrt(precision) for regression coefficient sqrt(precision)
  tau2_tau_beta <- pow(tau_tau_beta, 2)                # pooled precision for  regression coefficient sqrt(precision)
  p_ind[1] <- 1/2                                      # prior probability of nonzero coefficient
  p_ind[2] <- 1 - p_ind[1]                             # prior probability effectively zero coefficient

  for (j in 1:t){
    for(i in 1:p){
      indA[i, j] ~ dcat(p_ind[])                       # sample indicator for nonzero coefficient, returns 1 or 2 
      gamma[i, j] <- indA[i, j] - 1                    # transform indicator from 0-1 to 1-2 for indexing, returns 0 or 1
      beta[i, j] ~ dnorm(0, precision_beta[indA[i, j], j] / D[i])  # ssvs prior for regression coefficients
    }
  }
  
  # t degrees of freedom
  for(i in 1:t){
    nu_inv[i] ~ dbeta(alpha_nu, beta_nu)               # pooled inverse degrees of freedom for t data model
    nu[i] <- 2 / nu_inv[i]                             # pooled degrees of freedom for t data model
  }

  mu_nu ~ dbeta(5, 5)                                  # prior for alternative parameterization
  eta_nu ~ dgamma(10, 0.1)                             # prior for alternative parameterization
  alpha_nu <- mu_nu * eta_nu                           # transformation of prior
  beta_nu <- (1 - mu_nu) * eta_nu                      # transformation of prior

  # likelihood
  for (i in 1:N_total){
    Y[i] ~ dnorm(mu_y[i], 1 / v[i])                    # t mixture data model using precision
    log_like[i] <- dnorm(Y[i], mu_y[i], 1 / v[i])      # t mixture data likelihood using precision
  }

}
", file="./models-sens/tPCASSVS.txt")
```

```{r tPCALasso, echo=FALSE, eval=TRUE}
cat("model{
  ## hierarchically pooled data model sqrt(precision)
  for(i in 1:t){
    s_inv[i] ~ dlnorm(mu_s_inv, tau2_s)                # pooling prior for data model sqrt(precision)
    s2[i] <- pow(s_inv[i], -2)                         # data model precision
  }
  mu_s_inv ~ dnorm(0, 1 / 1)                         # pooled mean for data model sqrt(precision)
  tau_s ~ dunif(0, 1)                                # pooled sqrt(precision) for data model sqrt(precision)
  # mu_s_inv ~ dnorm(0, 1 / 100)                         # pooled mean for data model sqrt(precision)
  # tau_s ~ dunif(0, 100)                                # pooled sqrt(precision) for data model sqrt(precision)
  tau2_s <- pow(tau_s, 2)                              # pooled precision for data model sqrt(precision)

  for (i in 1:N_total){
    mu_y[i] <- X[H[i], ] %*% beta[, tt[i]]             # data model mean for sparse data format
                                                       # X <- PCA rotation matrix
                                                       # H <- observation indicator
    ## hierarchically pooled degress of freedom
    v_inv[i] ~ dchisq(nu[tt[i]])                       # scale mixture for t data model variance 
    v[i] <- (nu[tt[i]] * s2[tt[i]]) / v_inv[i]         # transformation to scaled inv-Chi squared variance
  }

  # lasso priors
  for(j in 1:t){  
    lambda2[j] ~ dgamma(alpha_lambda2, beta_lambda2)  # hierarchically pooled lasso shrinkage parameter
    for(i in 1:p){
      gamma2[i, j] ~ dexp(lambda2[j] / 2)             # lasso scale mixture parameter
      beta[i, j] ~ dnorm(0, 1 / (s2[j] * nu[j] / (nu[j] - 2) * gamma2[i, j] * D[i]))  # lasso prior model for tPCA 
                                                      # D is a vector of singular values from PCA
    }
  }
  mu_lambda2 ~ dlnorm(0, 1 / 1)                       # hierarchical mean of lambda2 gamma distribution
  s_lambda2 ~ dunif(0, 1)                            # hierarchical standard deviation of lambda2 gamma distribution
  # mu_lambda2 ~ dlnorm(0, 1 / 100)                       # hierarchical mean of lambda2 gamma distribution
  # s_lambda2 ~ dunif(0, 100)                            # hierarchical standard deviation of lambda2 gamma distribution
  alpha_lambda2 <- pow(mu_lambda2, 2) / pow(s_lambda2, 2) # reparameterization of gamma distribution
  beta_lambda2 <- mu_lambda2 / pow(s_lambda2, 2)       # reparameterization of gamma distribution

  # t degrees of freedom
  for(i in 1:t){
    nu_inv[i] ~ dbeta(alpha_nu, beta_nu)               # pooled inverse degrees of freedom for t data model
    nu[i] <- 2 / nu_inv[i]                             # pooled degrees of freedom for t data model
  }
  mu_nu ~ dbeta(5, 5)                                  # prior for alternative parameterization
  eta_nu ~ dgamma(10, 0.1)                             # prior for alternative parameterization
  alpha_nu <- mu_nu * eta_nu                           # transformation of prior
  beta_nu <- (1 - mu_nu) * eta_nu                      # transformation of prior

  # likelihood
  for (i in 1:N_total){
    Y[i] ~ dnorm(mu_y[i], 1 / v[i])                    # t mixture data model using precision
    log_like[i] <- dnorm(Y[i], mu_y[i], 1 / v[i])      # t mixture data likelihood using precision
  }

}
", file="./models-sens/tPCALasso.txt")
```

```{r pPCASSVS, echo=FALSE, eval=TRUE}
cat("model{
  ## hierarchically pooled data model variance
  for(j in 1:t){
    tau_inv[j] ~ dlnorm(mu_tau, tau2_tau)              # pooling prior for data model sqrt(precision)
    tau2[j] <- pow(tau_inv[j], -2)                     # data model precision
  }
  mu_tau ~ dnorm(0, 1 / 1)                           # pooled mean for data model sqrt(precision)
  tau_tau ~ dunif(0, 1)                              # pooled sqrt(precision) for data model sqrt(precision)
  # mu_tau ~ dnorm(0, 1 / 100)                           # pooled mean for data model sqrt(precision)
  # tau_tau ~ dunif(0, 100)                              # pooled sqrt(precision) for data model sqrt(precision)
  tau2_tau <- pow(tau_tau, 2)                          # pooled precision for data model sqrt(precision)
  ## latent variance model 
  s_inv ~ dunif(0, 10)                                # prior for latent principal component sqrt(precision)
  # s_inv ~ dunif(0, 100)                                # prior for latent principal component sqrt(precision)
  s2 <- pow(s_inv, -2)                                 # prior for latent principal component precision
  M_inv <- inverse(tKK / s2 + I_p)                     # scaling matrix for PCA

  for(i in 1:N_total){
    mu_y[i] <- X[H[i], ] %*% K_hat %*% M_inv %*% beta[, tt[i]] / s2
                                                      # data model mean for sparse data format
                                                      # X <- original data matrix
                                                      # K_hat <- PCA rotation matrix
                                                      # H <- observation indicator
  }
  for(i in 1:t){
    tau_y[i] <- pow(tau2[i] + t(beta[, i]) %*% M_inv %*% beta[, i], -1)
                                                      # data model precision
  }

  # hierarchically pooled ssvs priors
  for(i in 1:t){
    tau_beta[i] ~ dlnorm(mu_tau_beta, tau2_tau_beta)   # regression coefficient sqrt(precision)
    tau2_beta[i] <- pow(tau_beta[i], 2)                # regression coefficient precision
    precision_beta[1, i] <- tau2_beta[i]               # regression coefficient effectively zero
    precision_beta[2, i] <- tau2_beta[i] / 1000        # nonzero coefficient
  }
  mu_tau_beta ~ dnorm(0, 1 / 1)                      # pooled mean for regression coefficient sqrt(precision)
  tau_tau_beta ~ dunif(0, 1)                         # pooled sqrt(precision) for regression coefficient sqrt(precision)
  # mu_tau_beta ~ dnorm(0, 1 / 100)                      # pooled mean for regression coefficient sqrt(precision)
  # tau_tau_beta ~ dunif(0, 100)                         # pooled sqrt(precision) for regression coefficient sqrt(precision)
  tau2_tau_beta <- pow(tau_tau_beta, 2)                # pooled precision for  regression coefficient sqrt(precision)
  p_ind[1] <- 1/2                                      # prior probability of nonzero coefficient
  p_ind[2] <- 1 - p_ind[1]                             # prior probability effectively zero coefficient

  for (j in 1:t){
    for(i in 1:p){
      indA[i, j] ~ dcat(p_ind[])                       # sample indicator for nonzero coefficient, returns 1 or 2 
      gamma[i, j] <- indA[i, j] - 1                    # transform indicator from 0-1 to 1-2 for indexing, returns 0 or 1
      beta[i, j] ~ dnorm(0, precision_beta[indA[i, j], j])  # ssvs prior for regression coefficients
    }
  }

  # likelihood
  for (i in 1:N_total){
    Y[i] ~ dnorm(mu_y[i], tau_y[tt[i]])                # data model for sparse data format
  }
}
", file="./models-sens/pPCASSVS.txt")
```

```{r pPCALasso, echo=FALSE, eval=TRUE}
cat("model{
  ## hierarchically pooled data model variance
  for(j in 1:t){
    tau_inv[j] ~ dlnorm(mu_tau, tau2_tau)              # pooling prior for data model sqrt(precision)
    tau2[j] <- pow(tau_inv[j], -2)                     # data model precision
  }
  mu_tau ~ dnorm(0, 1 / 1)                           # pooled mean for data model sqrt(precision)
  tau_tau ~ dunif(0, 1)                              # pooled sqrt(precision) for data model sqrt(precision)
  # mu_tau ~ dnorm(0, 1 / 100)                           # pooled mean for data model sqrt(precision)
  # tau_tau ~ dunif(0, 100)                              # pooled sqrt(precision) for data model sqrt(precision)
  tau2_tau <- pow(tau_tau, 2)                          # pooled precision for data model sqrt(precision)
  ## latent variance model 
  s_inv ~ dunif(0, 10)                                # prior for latent principal component sqrt(precision)
  # s_inv ~ dunif(0, 100)                                # prior for latent principal component sqrt(precision)
  s2 <- pow(s_inv, -2)                                 # prior for latent principal component precision
  M_inv <- inverse(tKK / s2 + I_p)                     # scaling matrix for PCA

  for(i in 1:N_total){
    mu_y[i] <- X[H[i], ] %*% K_hat %*% M_inv %*% beta[, tt[i]] / s2
                                                      # data model mean for sparse data format
                                                      # X <- original data matrix
                                                      # K_hat <- PCA rotation matrix
                                                      # H <- observation indicator
  }
  for(i in 1:t){
    tau_y[i] <- pow(tau2[i] + t(beta[, i]) %*% M_inv %*% beta[, i], -1)
                                                      # data model precision
  }

  # lasso priors
  for(j in 1:t){  
    lambda2[j] ~ dgamma(alpha_lambda2, beta_lambda2)  # hierarchically pooled lasso shrinkage parameter
    for(i in 1:p){
      gamma2[i, j] ~ dexp(lambda2[j] / 2)             # lasso scale mixture parameter
      beta[i, j] ~ dnorm(0, 1 / (tau2[j] * gamma2[i, j]))  # lasso prior model for PCA 
    }
  }
  mu_lambda2 ~ dlnorm(0, 1 / 1)                       # hierarchical mean of lambda2 gamma distribution
  s_lambda2 ~ dunif(0, 1)                            # hierarchical standard deviation of lambda2 gamma distribution
  # mu_lambda2 ~ dlnorm(0, 1 / 100)                       # hierarchical mean of lambda2 gamma distribution
  # s_lambda2 ~ dunif(0, 100)                            # hierarchical standard deviation of lambda2 gamma distribution
  alpha_lambda2 <- pow(mu_lambda2, 2) / pow(s_lambda2, 2) # reparameterization of gamma distribution
  beta_lambda2 <- mu_lambda2 / pow(s_lambda2, 2)       # reparameterization of gamma distribution

  # likelihood
  for (i in 1:N_total){
      Y[i] ~ dnorm(mu_y[i], tau_y[tt[i]])              # data model for sparse data format
  }
}
", file="./models-sens/pPCALasso.txt")
```

```{r tpPCASSVS, echo=FALSE, eval=TRUE}
cat("model{
  ## hierarchically pooled data model variance
  for(j in 1:t){
    tau_inv[j] ~ dlnorm(mu_tau, tau2_tau)              # pooling prior for data model sqrt(precision)
    tau2[j] <- pow(tau_inv[j], -2)                     # data model precision
  }
  mu_tau ~ dnorm(0, 1 / 1)                           # pooled mean for data model sqrt(precision)
  tau_tau ~ dunif(0, 1)                              # pooled sqrt(precision) for data model sqrt(precision)
  # mu_tau ~ dnorm(0, 1 / 100)                           # pooled mean for data model sqrt(precision)
  # tau_tau ~ dunif(0, 100)                              # pooled sqrt(precision) for data model sqrt(precision)
  tau2_tau <- pow(tau_tau, 2)                          # pooled precision for data model sqrt(precision)
  ## latent variance model 
  s_inv ~ dunif(0, 10)                                # prior for latent principal component sqrt(precision)
  # s_inv ~ dunif(0, 100)                                # prior for latent principal component sqrt(precision)
  s2 <- pow(s_inv, -2)                                 # prior for latent principal component precision
  M_inv <- inverse(tKK / s2 + I_p)                     # scaling matrix for PCA

  for (i in 1:N_total){
    mu_y[i] <- X[H[i], ] %*% K_hat %*% M_inv %*% beta[, tt[i]] / s2
                                                      # data model mean for sparse data format
                                                      # X <- original data matrix
                                                      # K_hat <- PCA rotation matrix
                                                      # H <- observation indicator
    ## hierarchically pooled degress of freedom
    v_inv[i] ~ dchisq(nu[tt[i]])                      # scale mixture for t data model variance 
    v[i] <- (nu[tt[i]] * tau2[tt[i]]) / v_inv[i]      # transformation to scaled inv-Chi squared variance
    tau_y[i] <- pow(v[i] + t(beta[, tt[i]]) %*% M_inv %*% beta[, tt[i]], -1)
                                                      # data model precision for sparse data format
  }

  # hierarchically pooled ssvs priors
  for(i in 1:t){
    tau_beta[i] ~ dlnorm(mu_tau_beta, tau2_tau_beta)   # regression coefficient sqrt(precision)
    tau2_beta[i] <- pow(tau_beta[i], 2)                # regression coefficient precision
    precision_beta[1, i] <- tau2_beta[i]               # regression coefficient effectively zero
    precision_beta[2, i] <- tau2_beta[i] / 1000        # nonzero coefficient
  }
  mu_tau_beta ~ dnorm(0, 1 / 1)                      # pooled mean for regression coefficient sqrt(precision)
  tau_tau_beta ~ dunif(0, 1)                         # pooled sqrt(precision) for regression coefficient sqrt(precision)
  # mu_tau_beta ~ dnorm(0, 1 / 100)                      # pooled mean for regression coefficient sqrt(precision)
  # tau_tau_beta ~ dunif(0, 100)                         # pooled sqrt(precision) for regression coefficient sqrt(precision)
  tau2_tau_beta <- pow(tau_tau_beta, 2)                # pooled precision for  regression coefficient sqrt(precision)
  p_ind[1] <- 1/2                                      # prior probability of nonzero coefficient
  p_ind[2] <- 1 - p_ind[1]                             # prior probability effectively zero coefficient

  for (j in 1:t){
    for(i in 1:p){
      indA[i, j] ~ dcat(p_ind[])                       # sample indicator for nonzero coefficient, returns 1 or 2 
      gamma[i, j] <- indA[i, j] - 1                    # transform indicator from 0-1 to 1-2 for indexing, returns 0 or 1
      beta[i, j] ~ dnorm(0, precision_beta[indA[i, j], j])  # ssvs prior for regression coefficients
    }
  }
  
  # t degrees of freedom
  for(i in 1:t){

    nu_inv[i] ~ dbeta(alpha_nu, beta_nu)               # pooled inverse degrees of freedom for t data model
    nu[i] <- 2 / nu_inv[i]                             # pooled degrees of freedom for t data model
    # nu[i] ~ dgamma(2, 0.1)                             # unpooled degrees of freedom for t data model
  }
  mu_nu ~ dbeta(5, 5)                                  # prior for alternative parameterization
  eta_nu ~ dgamma(10, 0.1)                             # prior for alternative parameterization
  alpha_nu <- mu_nu * eta_nu                           # transformation of prior
  beta_nu <- (1 - mu_nu) * eta_nu                      # transformation of prior

  # likelihood
  for (i in 1:N_total){
    Y[i] ~ dnorm(mu_y[i], tau_y[i])                    # t mixture data model using precision
    log_like[i] <- dnorm(Y[i], mu_y[i], tau_y[i])      # t mixture data likelihood using precision 
  }
}
", file="./models-sens/tpPCASSVS.txt")
```

```{r tpPCALasso, echo=FALSE, eval=TRUE}
cat("model{
  ## hierarchically pooled data model variance
  for(j in 1:t){
    tau_inv[j] ~ dlnorm(mu_tau, tau2_tau)              # pooling prior for data model sqrt(precision)
    tau2[j] <- pow(tau_inv[j], -2)                     # data model precision
  }
  mu_tau ~ dnorm(0, 1 / 1)                           # pooled mean for data model sqrt(precision)
  tau_tau ~ dunif(0, 1)                              # pooled sqrt(precision) for data model sqrt(precision)
  # mu_tau ~ dnorm(0, 1 / 100)                           # pooled mean for data model sqrt(precision)
  # tau_tau ~ dunif(0, 100)                              # pooled sqrt(precision) for data model sqrt(precision)
  tau2_tau <- pow(tau_tau, 2)                          # pooled precision for data model sqrt(precision)
  ## latent variance model 
  s_inv ~ dunif(0, 10)                                # prior for latent principal component sqrt(precision)
  # s_inv ~ dunif(0, 100)                                # prior for latent principal component sqrt(precision)
  s2 <- pow(s_inv, -2)                                 # prior for latent principal component precision
  M_inv <- inverse(tKK / s2 + I_p)                     # scaling matrix for PCA

  for (i in 1:N_total){
    mu_y[i] <- X[H[i], ] %*% K_hat %*% M_inv %*% beta[, tt[i]] / s2
                                                      # data model mean for sparse data format
                                                      # X <- original data matrix
                                                      # K_hat <- PCA rotation matrix
                                                      # H <- observation indicator
    ## hierarchically pooled degress of freedom
    v_inv[i] ~ dchisq(nu[tt[i]])                      # scale mixture for t data model variance 
    v[i] <- (nu[tt[i]] * tau2[tt[i]]) / v_inv[i]      # transformation to scaled inv-Chi squared variance
    tau_y[i] <- pow(v[i] + t(beta[, tt[i]]) %*% M_inv %*% beta[, tt[i]], -1)
                                                      # data model precision for sparse data format
  }

  # lasso priors
  for(j in 1:t){  
    lambda2[j] ~ dgamma(alpha_lambda2, beta_lambda2)  # hierarchically pooled lasso shrinkage parameter
    for(i in 1:p){
      gamma2[i, j] ~ dexp(lambda2[j] / 2)             # lasso scale mixture parameter
      beta[i, j] ~ dnorm(0, 1 / (tau2[j] * nu[j] / (nu[j] - 2) * gamma2[i, j]))  # lasso prior model for tpPCA 
    }
  }

  mu_lambda2 ~ dlnorm(0, 1 / 1)                       # hierarchical mean of lambda2 gamma distribution
  s_lambda2 ~ dunif(0, 1)                            # hierarchical standard deviation of lambda2 gamma distribution
  # mu_lambda2 ~ dlnorm(0, 1 / 100)                       # hierarchical mean of lambda2 gamma distribution
  # s_lambda2 ~ dunif(0, 100)                            # hierarchical standard deviation of lambda2 gamma distribution
  alpha_lambda2 <- pow(mu_lambda2, 2) / pow(s_lambda2, 2) # reparameterization of gamma distribution
  beta_lambda2 <- mu_lambda2 / pow(s_lambda2, 2)       # reparameterization of gamma distribution

  # t degrees of freedom
  for(i in 1:t){
    nu_inv[i] ~ dbeta(alpha_nu, beta_nu)               # pooled inverse degrees of freedom for t data model
    nu[i] <- 2 / nu_inv[i]                             # pooled degrees of freedom for t data model
  }
  mu_nu ~ dbeta(5, 5)                                  # prior for alternative parameterization
  eta_nu ~ dgamma(10, 0.1)                             # prior for alternative parameterization
  alpha_nu <- mu_nu * eta_nu                           # transformation of prior
  beta_nu <- (1 - mu_nu) * eta_nu                      # transformation of prior

  # likelihood
  for (i in 1:N_total){
    Y[i] ~ dnorm(mu_y[i], tau_y[i])                    # t mixture data model using precision
    log_like[i] <- dnorm(Y[i], mu_y[i], tau_y[i])      # t mixture data likelihood using precision 
  }
}", file="./models-sens/tpPCALasso.txt")
```


## Fit the candidate models

```{r, cache=TRUE}
start_sim <- Sys.time()
```

```{r PCASSVSSim, echo=FALSE, eval=TRUE}
if (file.exists("./sim-fit/pcassvs.RData")) {
  load("./sim-fit/pcassvs.RData")
} else {
  ## fit mcmc
  jinits <- function(){
    list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(t, 0, 100))
  }
  
  dat <- list(Y=Y_sparse, X=X_PCA$X_pca[, 1:p], p=p, D=c(X_PCA$sdev^2),
              H=H_sparse, t=t, tt=tt, N_total=sum(nobs))
  
  params <- c("gamma", "beta", "s_inv", "tau2_beta")
  
  samps <- foreach(i=1:n_chains, .inorder = FALSE, 
                   .packages = c("rjags", "random"), .combine = "mcmc.combine", 
                   .multicombine = TRUE) %dopar% {
                     model.jags <- jags.model("./models-sens/PCASSVS.txt", data=dat,
                                              inits=jinits,
                                              n.chain=1, n.adapt=n_adapt)
                     update(model.jags, n_burn)
                     result <- coda.samples(model.jags, params, n_iter)
                     return(result)
                   }
  
  ## Diagnostic Checks
  make_diagnostics(samps, t=t, p=p, nobs=nobs, model="pca_ssvs", type = "sim")
  
  samps <- combine.mcmc(samps)
  n_samps <- dim(samps)[1]
  
  s_invs <- matrix(0, n_samps, t)
  betas <- array(0, dim=c(n_samps, t, p))
  for(i in 1:t){
    s_invs[, i] <- samps[, paste("s_inv[", i, "]", sep="")]
    for(j in 1:p){
      betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
    }
  }               
  
  summaries <- processPCASamples(t, p, n_samps, N, betas,
                                 s_invs, X_PCA$X_pca[, 1:p],
                                 Y_list, H_list, mu, nobs, n_thin)
  
  y_tilde_mean_pca_ssvs <- summaries$y_tilde_mean
  y_tilde_sd_pca_ssvs <- summaries$y_tilde_sd
  # crps_pca_ssvs <- makeCRPS(summaries$y_tilde, Z_hist, n_samps / n_thin)
  loo_pca_ssvs <- loo(matrix(unlist(summaries$log_like), n_samps / n_thin, 
                             sum(nobs), byrow=FALSE))
  rm(samps, betas, s_invs, summaries)
  save(y_tilde_mean_pca_ssvs, y_tilde_sd_pca_ssvs, loo_pca_ssvs, 
       file="./sim-fit/pcassvs.RData")
}
```

```{r PCALassoSim, echo=FALSE, eval=TRUE}
if (file.exists("./sim-fit/pcalasso.RData")) {
  load("./sim-fit/pcalasso.RData")
} else {
  ## fit mcmc

  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(t, 0, 100),
       gamma2=matrix(rgamma(p*t, 1, 1), p, t), lambda2 = rexp(t))
}

dat <- list(Y=Y_sparse, X=X_PCA$X_pca[, 1:p], p=p, H=H_sparse, t=t,
            tt=tt, N_total=sum(nobs), D=(X_PCA$sdev^2)[1:p])

params <- c("gamma2", "lambda2", "beta", "s_inv")

samps <- foreach(i=1:n_chains, .inorder = FALSE, 
                 .packages = c("rjags", "random"), .combine = "mcmc.combine", 
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/PCALasso.txt", 
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="pca_lasso", type="sim")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

s_invs <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
for(i in 1:t){
  s_invs[, i] <- samps[, paste("s_inv[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}               

summaries <- processPCASamples(t, p, n_samps, N, betas, s_invs,
                               X_PCA$X_pca[, 1:p], Y_list,
                               H_list, mu, nobs, n_thin)

y_tilde_mean_pca_lasso <- summaries$y_tilde_mean 
y_tilde_sd_pca_lasso <- summaries$y_tilde_sd
# crps_pca_lasso <- makeCRPS(summaries$y_tilde, Z_hist, n_samps / n_thin)
loo_pca_lasso <- loo(matrix(unlist(summaries$log_like), 
                            n_samps / n_thin, sum(nobs), byrow=FALSE))
rm(samps, summaries, betas, s_invs)
  save(y_tilde_mean_pca_lasso, y_tilde_sd_pca_lasso, loo_pca_lasso, 
       file="./sim-fit/pcalasso.RData")
}
```

```{r tPCASSVSSim, echo=FALSE, eval=TRUE}
if (file.exists("./sim-fit/tpcassvs.RData")) {
  load("./sim-fit/tpcassvs.RData")
} else {
  ## fit mcmc
  
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(t, 0, 100))
}

dat <- list(Y=Y_sparse, X=X_PCA$X_pca[, 1:p], p=p, D=c(X_PCA$sdev^2),
            H=H_sparse, t=t, tt=tt, N_total=sum(nobs))

params <- c("gamma", "beta", "s_inv", "tau2_beta", "nu", "v_inv", "log_like")

samps <- foreach(i=1:n_chains, .inorder = FALSE, 
                 .packages = c("rjags", "random"), .combine = "mcmc.combine", 
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/tPCASSVS.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="tpca_ssvs", type="sim")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

nus <- matrix(0, n_samps, t)
v_invs <- matrix(0, n_samps, sum(nobs))
s_invs <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
log_like <- matrix(0, n_samps, sum(nobs))
for(i in 1:t){
  s_invs[, i] <- samps[, paste("s_inv[", i, "]", sep="")]
  nus[, i] <- samps[, paste("nu[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}               
for(i in 1:sum(nobs)){
  log_like[, i] <- log(samps[, paste("log_like[", i, "]", sep="")])
  v_invs[, i] <- samps[, paste("v_inv[", i, "]", sep="")]
}    

summaries <- processPCASamplesT2(t, p, n_samps, N, betas, s_invs, nus, 
                                 v_invs, X_PCA$X_pca[, 1:p], Y_list,
                                 H_list, mu, nobs, n_thin)

y_tilde_mean_tpca_ssvs <- summaries$y_tilde_mean 
y_tilde_sd_tpca_ssvs <- summaries$y_tilde_sd
# crps_tpca_ssvs <- makeCRPS(summaries$y_tilde, Z_hist, n_samps / n_thin)
loo_tpca_ssvs <- loo(log_like)
rm(samps, nus, v_invs, s_invs, betas, log_like, summaries)
  save(y_tilde_mean_tpca_ssvs, y_tilde_sd_tpca_ssvs, loo_tpca_ssvs, 
       file="./sim-fit/tpcassvs.RData")
}
```

```{r tPCALassoSim, echo=FALSE, eval=TRUE}
if (file.exists("./sim-fit/tpcalasso.RData")) {
  load("./sim-fit/tpcalasso.RData")
} else {
  ## fit mcmc
  
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(t, 0, 100),
       gamma2=matrix(rgamma(p*t, 1, 1), p, t), lambda2=rexp(t))
}

dat <- list(Y=Y_sparse, X=X_PCA$X_pca[, 1:p], p=p, H=H_sparse, t=t,
            tt=tt, N_total=sum(nobs), D=(X_PCA$sdev^2)[1:p])

params <- c("gamma2", "lambda2", "beta", "v_inv", "nu", "s_inv", "log_like")

samps <- foreach(i=1:n_chains, .inorder = FALSE, 
                 .packages = c("rjags", "random"), .combine = "mcmc.combine", 
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/tPCALasso.txt", 
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 } 

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="tpca_lasso", type="sim")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

nus <- matrix(0, n_samps, t)
v_invs <- matrix(0, n_samps, sum(nobs))
s_invs <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
log_like <- matrix(0, n_samps, sum(nobs))
for(i in 1:t){
  s_invs[, i] <- samps[, paste("s_inv[", i, "]", sep="")]
  nus[, i] <- samps[, paste("nu[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}               
for(i in 1:sum(nobs)){
  log_like[, i] <- log(samps[, paste("log_like[", i, "]", sep="")])
  v_invs[, i] <- samps[, paste("v_inv[", i, "]", sep="")]
}

summaries <- processPCASamplesT2(t, p, n_samps, N, betas, s_invs, nus, v_invs,
                                 X_PCA$X_pca[, 1:p], Y_list,
                                 H_list, mu, nobs, n_thin)

y_tilde_mean_tpca_lasso <- summaries$y_tilde_mean 
y_tilde_sd_tpca_lasso <- summaries$y_tilde_sd
# crps_tpca_lasso <- makeCRPS(summaries$y_tilde, Z_hist, n_samps / n_thin)
loo_tpca_lasso <- loo(log_like)
rm(samps, nus, v_invs, s_invs, betas, log_like, summaries)
  save(y_tilde_mean_tpca_lasso, y_tilde_sd_tpca_lasso, loo_tpca_lasso, 
       file="./sim-fit/tpcalasso.RData")
}
```

```{r pPCASSVSSim, echo=FALSE, eval=TRUE}
if (file.exists("./sim-fit/ppcassvs.RData")) {
  load("./sim-fit/ppcassvs.RData")
} else {
  ## fit mcmc
  
  jinits <- function(){
  list(s_beta_inv=runif(1, 0, 10), beta=matrix(rnorm(p, 0, 10), p, t),
       tau_inv=runif(t, 0, 10), s_inv=runif(1, 0, 1))
}

dat <- list(Y=Y_sparse, X=X_PPCA$X_center, H=H_sparse, p=p, K_hat=X_PPCA$K_hat,
            tKK=X_PPCA$Lambda, I_p=diag(p), t=t, tt=tt, N_total=sum(nobs))

params <- c("gamma", "beta", "tau2", "s2", "tau2_beta")

samps <- foreach(i=1:n_chains, .inorder = FALSE, 
                 .packages = c("rjags", "random"), .combine = "mcmc.combine", 
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/pPCASSVS.txt", data=dat,
                                            inits=jinits,
                                            n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="ppca_ssvs", type="sim")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

tau2s <- matrix(0, n_samps, t)
s2s <- c(samps[, "s2"])
betas <- array(0, dim=c(n_samps, t, p))
for(i in 1:t){
  tau2s[, i] <- samps[, paste("tau2[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}               

summaries <- processSamples(t, p, n_samps, N, betas, tau2s, s2s, Y_list, 
                            H_list, nobs, X_PPCA$X_center, X_PPCA$Lambda, 
                            diag(p), X_PPCA$K_hat, mu, n_thin)

y_tilde_mean_ppca_ssvs <- summaries$y_tilde_mean
y_tilde_sd_ppca_ssvs <- summaries$y_tilde_sd
# crps_ppca_ssvs <- makeCRPS(summaries$y_tilde, Z_hist, n_samps / n_thin)
loo_ppca_ssvs <- loo(matrix(unlist(summaries$log_like), n_samps / n_thin,
                            sum(nobs), byrow=FALSE))
rm(samps, summaries, betas, s2s, tau2s)
  save(y_tilde_mean_ppca_ssvs, y_tilde_sd_ppca_ssvs, loo_ppca_ssvs, 
       file="./sim-fit/ppcassvs.RData")
}
```

```{r pPCALassoSim, echo=FALSE, eval=TRUE}
if (file.exists("./sim-fit/ppcalasso.RData")) {
  load("./sim-fit/ppcalasso.RData")
} else {
  ## fit mcmc
  
  jinits <- function(){
  list(beta=matrix(rnorm(p * t, 0, 10), p, t), tau_inv=runif(t, 0, 100), 
       s_inv=runif(1, 0, 10))
}

dat <- list(Y=Y_sparse, X=X_PPCA$X_center, H=H_sparse, p=p, K_hat=X_PPCA$K_hat,
            tKK=X_PPCA$Lambda, I_p=diag(p), t=t, tt=tt, N_total=sum(nobs))

params <- c("gamma2", "lambda2", "beta", "tau2", "s2")

samps <- foreach(i=1:n_chains, .inorder = FALSE, 
                 .packages = c("rjags", "random"), .combine = "mcmc.combine", 
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/pPCALasso.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="ppca_lasso", type="sim")


samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

tau2s <- matrix(0, n_samps, t)
s2s <- c(samps[, "s2"])
betas <- array(0, dim=c(n_samps, t, p))
for(i in 1:t){
  tau2s[, i] <- samps[, paste("tau2[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}               

summaries <- processSamples(t, p, n_samps, N, betas, tau2s, s2s, Y_list, H_list,
                            nobs, X_PPCA$X_center,
                            X_PPCA$Lambda, diag(p),
                            X_PPCA$K_hat, mu, n_thin)

y_tilde_mean_ppca_lasso <- summaries$y_tilde_mean
y_tilde_sd_ppca_lasso <- summaries$y_tilde_sd
# crps_ppca_lasso <- makeCRPS(summaries$y_tilde, Z_hist, n_samps / n_thin)
loo_ppca_lasso <- loo(matrix(unlist(summaries$log_like), 
                             n_samps / n_thin, sum(nobs), byrow=FALSE))
rm(samps, summaries, betas, s2s, tau2s)
  save(y_tilde_mean_ppca_lasso, y_tilde_sd_ppca_lasso, loo_ppca_lasso, 
       file="./sim-fit/ppcalasso.RData")
}
```

```{r tpPCASSVSSim, echo=FALSE, eval=TRUE}
if (file.exists("./sim-fit/tppcassvs.RData")) {
  load("./sim-fit/tppcassvs.RData")
} else {
  ## fit mcmc
  
  jinits <- function(){   
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(1, 0, 10), 
       tau_inv=runif(t, 0, 100)) 
} 

dat <- list(Y=Y_sparse, X=X_PPCA$X_center, H=H_sparse, p=p, K_hat=X_PPCA$K_hat,
            tKK=X_PPCA$Lambda, I_p=diag(p), t=t, tt=tt, N_total=sum(nobs))

params <- c("gamma", "beta", "s2", "tau2_beta", "nu", "v_inv", "log_like",
            "tau2", "alpha_nu", "beta_nu")

samps <- foreach(i=1:n_chains, .inorder = FALSE, 
                 .packages = c("rjags", "random"), .combine = "mcmc.combine", 
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/tpPCASSVS.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostics
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="tppca_ssvs", type="sim")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

nus <- matrix(0, n_samps, t)
v_invs <- matrix(0, n_samps, sum(nobs))
s2s <- samps[, paste("s2")]
tau2s <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
log_like <- matrix(0, n_samps, sum(nobs))
for(i in 1:t){
  tau2s[, i] <- samps[, paste("tau2[", i, "]", sep="")]
  nus[, i] <- samps[, paste("nu[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}               
for(i in 1:sum(nobs)){
  log_like[, i] <- log(samps[, paste("log_like[", i, "]", sep="")]) 
  v_invs[, i] <- samps[, paste("v_inv[", i, "]", sep="")] 
}      

summaries <- processSamplesT(t, p, n_samps, N, betas, tau2s, s2s, nus,
                             v_invs, Y_list, H_list, nobs, X_PPCA$X_center, 
                             X_PPCA$Lambda, diag(p), X_PPCA$K_hat, mu, n_thin)

y_tilde_mean_tppca_ssvs <- summaries$y_tilde_mean 
y_tilde_sd_tppca_ssvs <- summaries$y_tilde_sd
# crps_tppca_ssvs <- makeCRPS(summaries$y_tilde, Z_hist, n_samps / n_thin)
loo_tppca_ssvs <- loo(log_like)
rm(samps, nus, v_invs, s2s, tau2s, betas, log_like, summaries)
  save(y_tilde_mean_tppca_ssvs, y_tilde_sd_tppca_ssvs, loo_tppca_ssvs, 
       file="./sim-fit/tppcassvs.RData")
}
```

```{r tpPCALassoSim, echo=FALSE, eval=TRUE}
if (file.exists("./sim-fit/pcalasso.RData")) {
  load("./sim-fit/tppcalasso.RData")
} else {
  ## fit mcmc
  
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t),  s_inv=runif(1, 0, 10),
       tau_inv=runif(t, 0, 100), gamma2=matrix(rgamma(p*t, 1, 1), p, t),
       lambda2=rexp(t))
}

dat <- list(Y=Y_sparse, X=X_PPCA$X_center, H=H_sparse, p=p, K_hat=X_PPCA$K_hat,
            tKK=X_PPCA$Lambda, I_p=diag(p), t=t, tt=tt, N_total=sum(nobs))

params <- c("gamma2", "lambda2", "beta", "nu", "s2", "tau2", "v_inv",
            "log_like")

samps <- foreach(i=1:n_chains, .inorder = FALSE, 
                 .packages = c("rjags", "random"), .combine = "mcmc.combine", 
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/tpPCALasso.txt", 
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostics
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="tppca_lasso", type="sim")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

nus <- matrix(0, n_samps, t)
v_invs <- matrix(0, n_samps, sum(nobs))
s2s <- samps[, paste("s2")]
tau2s <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
log_like <- matrix(0, n_samps, sum(nobs))
for(i in 1:t){
  tau2s[, i] <- samps[, paste("tau2[", i, "]", sep="")]
  nus[, i] <- samps[, paste("nu[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}               
for(i in 1:sum(nobs)){
  log_like[, i] <- log(samps[, paste("log_like[", i, "]", sep="")]) 
  v_invs[, i] <- samps[, paste("v_inv[", i, "]", sep="")]
}

summaries <- processSamplesT(t, p, n_samps, N, betas, tau2s, s2s, nus, v_invs,
                             Y_list, H_list, nobs, X_PPCA$X_center,
                             X_PPCA$Lambda, diag(p), X_PPCA$K_hat, mu, n_thin)

y_tilde_mean_tppca_lasso <- summaries$y_tilde_mean 
y_tilde_sd_tppca_lasso <- summaries$y_tilde_sd
# crps_tppca_lasso <- makeCRPS(summaries$y_tilde, Z_hist, n_samps / n_thin)
loo_tppca_lasso <- loo(log_like)
rm(samps, nus, v_invs, s2s, tau2s, betas, log_like, summaries)
  save(y_tilde_mean_tppca_lasso, y_tilde_sd_tppca_lasso, loo_tppca_lasso, 
       file="./sim-fit/tppcalasso.RData")
}
```

## Process Simulated Results

```{r setupTable, echo=FALSE, include=FALSE, message=FALSE, eval=TRUE}
# PCR Model
mspe_pca_ssvs <- mean((y_tilde_mean_pca_ssvs - Z_hist)^2)
mspe_pca_lasso <- mean((y_tilde_mean_pca_lasso - Z_hist)^2)
mspe_tpca_ssvs <- mean((y_tilde_mean_tpca_ssvs-Z_hist)^2)
mspe_tpca_lasso <- mean((y_tilde_mean_tpca_lasso-Z_hist)^2)

# crps_pca_ssvs <- mean(crps_pca_ssvs)
# crps_pca_lasso <- mean(crps_pca_lasso)
# crps_tpca_ssvs <- mean(crps_tpca_ssvs)
# crps_tpca_lasso <- mean(crps_tpca_lasso)

## probabilistic PCA
mspe_ppca_ssvs <- mean((y_tilde_mean_ppca_ssvs - Z_hist)^2)
mspe_ppca_lasso <- mean((y_tilde_mean_ppca_lasso - Z_hist)^2)
mspe_tppca_ssvs <- mean((y_tilde_mean_tppca_ssvs-Z_hist)^2)
mspe_tppca_lasso <- mean((y_tilde_mean_tppca_lasso-Z_hist)^2)

# crps_ppca_ssvs <- mean(crps_ppca_ssvs)
# crps_ppca_lasso <- mean(crps_ppca_lasso)
# crps_tppca_ssvs <- mean(crps_tppca_ssvs)
# crps_tppca_lasso <- mean(crps_tppca_lasso)
```

```{r simTablesXtable, echo=FALSE, message=FALSE, eval=TRUE}
MSPE <- data.frame(cbind(c(mspe_pca_ssvs, mspe_pca_lasso),
                         c(mspe_ppca_ssvs, mspe_ppca_lasso),
                         c(mspe_tpca_ssvs, mspe_tpca_lasso),
                         c(mspe_tppca_ssvs, mspe_tppca_lasso)))
rownames(MSPE) <- c("SSVS", "LASSO")
colnames(MSPE) <- c("PCR", "pPCR", "PCR", "pPCR")

# CRPS <- data.frame(cbind(c(crps_pca_ssvs, crps_pca_lasso),
#                          c(crps_ppca_ssvs, crps_ppca_lasso), 
#                          c(crps_tpca_ssvs, crps_tpca_lasso),
#                          c(crps_tppca_ssvs, crps_tppca_lasso)))
# rownames(CRPS) <- c("SSVS", "LASSO")
# colnames(CRPS) <- c("PCR", "pPCR", "PCR", "pPCR")


loo_ic <- data.frame(cbind(c(loo_pca_ssvs$looic,
                             loo_pca_lasso$looic),
                           c(loo_ppca_ssvs$looic,
                             loo_ppca_lasso$looic), 
                           c(loo_tpca_ssvs$looic,
                             loo_tpca_lasso$looic),
                           c(loo_tppca_ssvs$looic,
                             loo_tppca_lasso$looic)))
rownames(loo_ic) <- c("SSVS", "LASSO")
colnames(loo_ic) <- c("PCR", "pPCR", "PCR", "pPCR")
addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- c("\\multicolumn{1}{|r|}{} & \\multicolumn{2}{c|}{Gaussian} & \\multicolumn{2}{c|}{Robust} \\\\
& PCR & pPCR & PCR & pPCR \\\\\n")
print(xtable(MSPE, digits=4, align=c("|r|rr|rr|")),
      hline.after=c(-1, 0, nrow(MSPE)), add.to.row=addtorow, file="mspe.tex",
      floating=FALSE, size="small", include.colnames=FALSE)
# print(xtable(MSPE, digits=4), file="mspe.tex", floating=FALSE)
# print(xtable(CRPS, digits=1, align=c("|r|rr|rr|")), 
#       hline.after=c(-1, 0, nrow(CRPS)), add.to.row=addtorow, file="crps.tex",
#       floating=FALSE, size="small", include.colnames=FALSE)
# print(xtable(CRPS, digits=1), file="crps.tex", floating=FALSE)
print(xtable(loo_ic, digits=0, align=c("|r|rr|rr|")), 
      hline.after=c(-1, 0, nrow(loo_ic)), add.to.row=addtorow, file="loo.tex",
      floating=FALSE, size="small", include.colnames=FALSE)
# print(xtable(loo_ic, digits=0), file="loo.tex", floating=FALSE)


simscores <- data.frame(cbind(c(mspe_pca_ssvs, mspe_ppca_ssvs,
                                mspe_pca_lasso, mspe_ppca_lasso),
                              c(mspe_tpca_ssvs, mspe_tppca_ssvs,
                                mspe_tpca_lasso, mspe_tppca_lasso),
                              # c(crps_pca_ssvs, crps_ppca_ssvs,
                              #   crps_pca_lasso, crps_ppca_lasso), 
                              # c(crps_tpca_ssvs, crps_tppca_ssvs,
                              #   crps_tpca_lasso,  crps_tppca_lasso),
                              c(loo_pca_ssvs$looic, loo_ppca_ssvs$looic,
                                loo_pca_lasso$looic, loo_ppca_lasso$looic), 
                              c(loo_tpca_ssvs$looic, loo_tppca_ssvs$looic,
                                loo_tpca_lasso$looic, loo_tppca_lasso$looic)
))
rownames(simscores) <- c("SSVS PCR", "SSVS pPCR", "LASSO PCR", "LASSO pPCR")
addtorow <- list()
addtorow$pos <- list(0)
# addtorow$command <- c("\\multicolumn{1}{r}{} & \\multicolumn{2}{c}{MSPE} & \\multicolumn{2}{c}{CRPS} & \\multicolumn{2}{c}{LOO} \\\\
#  Model & Gaussian & Robust & Gaussian & Robust & Gaussian & Robust \\\\\n")
addtorow$command <- c("\\multicolumn{1}{r}{} & \\multicolumn{2}{c}{MSPE} &  \\multicolumn{2}{c}{LOO} \\\\
 Model & Gaussian & Robust & Gaussian & Robust \\\\\n")
print(xtable(simscores, align=c("rrrrr"), digits = c(0, 3, 3, 0, 0)),
# print(xtable(simscores, align=c("rrrrrrr"), digits = c(0, 3, 3, 0, 0, 0, 0)),
      hline.after=c(-1, 0, nrow(simscores)), add.to.row=addtorow, 
      file="sim-scores.tex",
      floating=FALSE, size="small", include.colnames=FALSE)

```

\begin{table}
\centering
{\small
\input{./sim-scores}
}
\caption{Simulation experiment scores. Smaller values indicate better model performance.}
\label{tab:simscore}
\end{table}




```{r looPlots, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE, fig.height=6, fig.width=12, eval=TRUE}
## PCA
par(mfrow=c(4, 2), mar=c(3, 3, 1, 1) + 0.1, oma=c(0, 0, 0, 0) + 0.1, mgp=c(2, 1, 0))
plot_k(loo_pca_ssvs$pareto_k, main="SSVS PCR")
plot_k(loo_pca_lasso$pareto_k, main="LASSO PCR")
plot_k(loo_tpca_ssvs$pareto_k, main="SSVS robust PCR")
plot_k(loo_tpca_lasso$pareto_k, main="LASSO robust PCR")
plot_k(loo_ppca_ssvs$pareto_k, main="SSVS pPCR")
plot_k(loo_ppca_lasso$pareto_k, main="LASSO pPCR")
plot_k(loo_tppca_ssvs$pareto_k, main="SSVS robust pPCR")
plot_k(loo_tppca_lasso$pareto_k, main="LASSO robust pPCR")

```

## Plot Simulation results

```{r pPCAreconstruction, echo=FALSE, include=FALSE, echo=FALSE, fig.height=6, fig.width=12, eval=TRUE}
par(mfrow=c(1, 2), mar=c(3, 3, 1, 1) + 0.1, oma=c(0, 0, 0, 0) + 0.1, mgp=c(2, 1, 0))
library(RColorBrewer)
c25 <- c("dodgerblue2","#E31A1C", # red
         "green4",
         "#6A3D9A", # purple
         "#FF7F00", # orange
         "black","gold1",
         "skyblue2","#FB9A99", # lt pink
         "palegreen2",
         "#CAB2D6", # lt purple
         "#FDBF6F", # lt orange
         "gray70", "khaki2",
         "maroon","orchid1","deeppink1","blue1","steelblue4",
         "darkturquoise","green1","yellow4","yellow3",
         "darkorange4","brown")
# matplot(Z_hist_noisy, type = 'l', main = 'Noisy observed data', ylab = "Temperature")
# matplot(Z_hist, type = 'l', main = 'Simulation truth', ylab = "Temperature")
plot(NA, type='n', xlim=c(min(y_tilde_mean_tpca_lasso), max(y_tilde_mean_tpca_lasso)), 
     ylim=c(min(Z_hist), max(Z_hist)), main = 'Robust PCR LASSO Reconstruction', xlab="Predicted", 
     ylab="Observed")
for (i in 1:dim(Z_hist)[2]) {
  points(y_tilde_mean_tpca_lasso[, i] ~ Z_hist[, i], col=adjustcolor(c25[i], alpha.f=0.25))
}
abline(0, 1)
plot(NA, type='n', xlim=c(min(y_tilde_mean_tppca_lasso), max(y_tilde_mean_tppca_lasso)), 
     ylim=c(min(Z_hist), max(Z_hist)), main = 'Robust pPCR LASSO Reconstruction', xlab="Predicted", 
     ylab="Observed")
for (i in 1:dim(Z_hist)[2]) {
  points(y_tilde_mean_tppca_lasso[, i] ~ Z_hist[, i], col=adjustcolor(c25[i], alpha.f=0.25))
}
abline(0, 1)
```

\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{observer-sensitivity_files/figure-latex/pPCAreconstruction-1.pdf}
\caption{Simulation truth plotted against predicted temperature for two models.}
\label{fig:recon}
\end{figure}

```{r pPCAreconstructionSurfaces, echo=FALSE, include=FALSE, echo=FALSE, fig.height=6, fig.width=12, eval=TRUE}
par(mfrow=c(4, 1), mar=c(3, 3, 1, 1) + 0.1, oma=c(0, 0, 0, 0) + 0.1, mgp=c(2, 1, 0))
matplot(Z_hist_noisy, type = 'l', main = 'Noisy observed data', ylab = "Temperature")
matplot(Z_hist, type = 'l', main = 'Simulation truth', ylab = "Temperature")
matplot(y_tilde_mean_tpca_lasso, type = 'l', ylab = "Temperature",
        main = 'Robust PCR LASSO Reconstruction')
matplot(y_tilde_mean_tppca_lasso, type = 'l', ylab = "Temperature", 
        main = 'Robust pPCR LASSO Reconstruction')
```

\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{observer-sensitivity_files/figure-latex/pPCAreconstructionSurfaces-1.pdf}
\caption{Simulation truth, noisy observations, and reconstructed surfaces for two models. Not included in manuscript.}
\label{fig:recon}
\end{figure}

```{r, cache=TRUE}
finish_sim <- Sys.time()
sim_time <- finish_sim - start_sim
sim_time
```

```{r cleanUp, echo=FALSE, eval=TRUE}
## clear variables from simulation study
rm(X, Y_list, Y_sparse, Y_center, H_list, H_sparse, mu, t, X_PCA, X_PPCA, locs)
n_chains <- 4
```

## Load observer/fort data

```{r loadFortData, echo=FALSE, eval=TRUE}

## Load updated data
load("../data/observer.RData")
X.farenheit <- (X.celsius * 9 / 5 + 32)

N <- dim(X.farenheit)[1]
t <- length(Y.list)
## Convert updated data to Farenheit
for (i in 1:t) {
  Y.list[[i]] <- (Y.list[[i]] * 9 / 5 + 32)
}
nobs <- rep(0, t)
d <- dim(X.farenheit)[2]
p <- 24
nobs <- rep(0, t)
mu <- rep(0, t)
Y_center <- Y.list
for(i in 1:t){
  nobs[i] <- length(Y.list[[i]])
  mu[i] <- mean(Y.list[[i]])
  Y_center[[i]] <- Y.list[[i]] - mu[i]
}

Y_sparse <- unlist(Y_center)
H_sparse <- unlist(H.list)
tt <- rep(1:t, nobs)
n_thin <- 10
I_t <- array(0, dim=c(t, max(nobs), max(nobs)))

pos_idx <- rep(1, t+1)
for(i in 2:(t+1)){
  pos_idx[i] <- pos_idx[i-1] + nobs[i-1];
}

X_PCA <- makePCA(X.farenheit)
X_PPCA <- makePPCA(X.farenheit, p)
```

## Plot Observer Data

```{r data-plot, include=FALSE, echo=FALSE, fig.height=6, fig.width=12, eval=TRUE}
library(RColorBrewer)

set.seed(401)
subset <- c(12, 28, 56, 71)

set.seed(401)
subset_prism <- sort(sample(1:116, 4))

min_prism <- rep(0, length(subset_prism))
max_prism <- rep(0, length(subset_prism))
k <- 0
for(i in subset_prism){
  k <- k + 1
  min_prism[k] <- min(values(prism[[i]])[ - na.rows]) / 100 * 9 / 5 + 32
  max_prism[k] <- max(values(prism[[i]])[ - na.rows]) / 100 * 9 / 5 + 32
}

# par(mfrow=c(2, 4), mar=c(0.5, 0.5, 1.75, 0.5) + 0.1, oma=c(0, 0, 0.25, 2.5) + 0.1)
layout(matrix(c(1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 9,
                1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 9,
                1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 9,
                5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9,
                5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9,
                5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9), 6, 13, byrow=TRUE))
par(mar=c(3.5, 3.25, 1.75, 0.5) + 0.1, oma=c(0, 0, 0.25, 0.25) + 0.1)
for(i in subset[1:2]){
  # palette(topo.colors(64))
  idx <- rep(0, length = length((values(fort.raster[[i]])[ - na.rows][H.list[[i]]])))
  for(j in 1:length((values(fort.raster[[i]])[ - na.rows][H.list[[i]]]))){
    idx[j] <- findInterval(((values(fort.raster[[i]])[ - na.rows][H.list[[i]]]))[j] * 9 / 5 + 32,
                           seq(min(min_prism), max(max_prism), length = 64))
  }
  if (i == subset[1]) {
    image((fort.raster[[i]]), main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(min(min_prism), max(max_prism)), ylab = '', xlab='',
          xaxt="n", cex.main=2.5)
    mtext("latitude", 2, line=2)
  } else {

    image((fort.raster[[i]]), main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(min(min_prism), max(max_prism)), ylab = '', xlab='',
          xaxt="n", yaxt='n', cex.main=2.5)
  }
  points(prism.points[H.list[[i]], ], pch = 16,
         col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64))[idx],
         cex = 1.5)
  map(database = 'state', add = TRUE, col = 'black')
  if(i == subset[1]){
    mtext("(a)", side=3, line=-1.9, adj=c(0.1), cex=2, col="black")
  }
}

for(i in subset_prism[1:2]){
  image(prism[[i]] / 100 * 9 / 5 + 32, main = paste(i + 1894),
        # col = topo.colors(64),
        col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
        zlim = c(min(min_prism), max(max_prism)), ylab = '', xlab='',
        yaxt="n", xaxt="n", cex.main=2.5)
  if(i == subset_prism[1]){
    mtext("(b)", side=3, line=-1.9, adj=c(0.1), cex=2, col="black")
  }
  map(database = 'state', add = TRUE, col = 'black')
}

for(i in subset[3:4]){
  # palette(topo.colors(64))
  idx <- rep(0, length = length((values(fort.raster[[i]])[ - na.rows][H.list[[i]]])))
  for(j in 1:length((values(fort.raster[[i]])[ - na.rows][H.list[[i]]]))){
    idx[j] <- findInterval(((values(fort.raster[[i]])[ - na.rows][H.list[[i]]]))[j] * 9 / 5 + 32,
                           seq(min(min_prism), max(max_prism), length = 64))
  }
  if (i == subset[3]) {
    image((fort.raster[[i]]), main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(min(min_prism), max(max_prism)), ylab = '', xlab='',
          cex.main=2.5)
    mtext("latitude", 2, line=2)
    mtext("longitude", 1, line=2.25)
  } else {
    image((fort.raster[[i]]), main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(min(min_prism), max(max_prism)), ylab = '', xlab='', yaxt='n',
          cex.main=2.5)
    mtext("longitude", 1, line=2.25)
  }
  points(prism.points[H.list[[i]], ], pch = 16,
         col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64))[idx],
         # col = topo.colors(64)[idx],
         cex = 1.5)
  map(database = 'state', add = TRUE, col = 'black')
}

for(i in subset_prism[3:4]){
  image(prism[[i]] / 100 * 9 / 5 + 32, main = paste(i + 1894),
        col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
        #col = topo.colors(64),
        zlim = c(min(min_prism), max(max_prism)), ylab = '', xlab='',
        yaxt="n", cex.main=2.5)
  mtext("longitude", 1, line=2.25)
  map(database = 'state', add = TRUE, col = 'black')
}
image.scale(prism[[i]], col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
            #col = topo.colors(64),
            zlim = c(min(min_prism), max(max_prism)), horiz=FALSE)
```

\begin{figure}
\centering
\centering\includegraphics[width=0.95\linewidth]{observer-sensitivity_files/figure-latex/data-plot-1.pdf}
\caption{Four representative years of the historical fort temperature data (a) and the observational PRISM temperature data (b).}
\label{fig:plotdata}
\end{figure}

## Fit candidate models

```{r, cache=TRUE}
start_observer <- Sys.time()
```

```{r PCASSVSObserver, echo=FALSE, eval=TRUE}
if (file.exists("./observer-fit/pcassvs.RData")) {
  load("./observer-fit/pcassvs.RData")
} else {
  ## fit mcmc
  jinits <- function(){
    list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(t, 0, 100))
  }
  
  dat <- list(Y=Y_sparse, X=X_PCA$X_pca[, 1:p], p=p, D=c(X_PCA$sdev^2),
              H=H_sparse, t=t, tt=tt, N_total=sum(nobs))
  
  params <- c("gamma", "beta", "s_inv", "tau2_beta")
  
  samps <- foreach(i=1:n_chains, .inorder = FALSE,
                   .packages = c("rjags", "random"), .combine = "mcmc.combine",
                   .multicombine = TRUE) %dopar% {
                     model.jags <- jags.model("./models-sens/PCASSVS.txt",
                                              data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                     update(model.jags, n_burn)
                     result <- coda.samples(model.jags, params, n_iter)
                     return(result)
                   }
  
  ## Diagnostic Checks
  make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="pca_ssvs", type = "observer")
  
  
  samps <- combine.mcmc(samps)
  n_samps <- dim(samps)[1]
  
  s_invs <- matrix(0, n_samps, t)
  betas <- array(0, dim=c(n_samps, t, p))
  for(i in 1:t){
    s_invs[, i] <- samps[, paste("s_inv[", i, "]", sep="")]
    for(j in 1:p){
      betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
    }
  }
  
  summaries <- processPCASamples(t, p, n_samps, N, betas, s_invs,
                                 X_PCA$X_pca[, 1:p], Y.list, H.list, mu, nobs,
                                 n_thin, preds=FALSE)
  
  y_tilde_mean_pca_ssvs_fort <- summaries$y_tilde_mean
  y_tilde_sd_pca_ssvs_fort <- summaries$y_tilde_sd
  loo_pca_ssvs_fort <- loo(matrix(unlist(summaries$log_like),
                                  n_samps / n_thin, sum(nobs), byrow=FALSE))
  rm(samps, summaries, betas, s_invs)
  save(y_tilde_mean_pca_ssvs_fort, y_tilde_sd_pca_ssvs_fort, loo_pca_ssvs_fort, 
       file="./observer-fit/pcassvs.RData")
}
```

```{r PCALassoObserver, echo=FALSE, eval=TRUE}
if (file.exists("./observer-fit/pcalasso.RData")) {
  load("./observer-fit/pcalasso.RData")
} else {
  ## fit mcmc

  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(t, 0, 100),
       gamma2=matrix(rgamma(p*t, 1, 1), p, t), lambda2 = rexp(t))
}

dat <- list(Y=Y_sparse, X=X_PCA$X_pca[, 1:p], p=p, H=H_sparse, t=t, tt=tt,
            N_total=sum(nobs), D=c(X_PCA$sdev^2))

params <- c("beta", "s_inv", "gamma2", "lambda2")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/PCALasso.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="pca_lasso", type="observer")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

s_invs <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
for(i in 1:t){
  s_invs[, i] <- samps[, paste("s_inv[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}

summaries <- processPCASamples(t, p, n_samps, N, betas, s_invs,
                               X_PCA$X_pca[, 1:p], Y.list, H.list,
                               mu, nobs, n_thin, preds=FALSE)

y_tilde_mean_pca_lasso_fort <- summaries$y_tilde_mean
y_tilde_sd_pca_lasso_fort <- summaries$y_tilde_sd
loo_pca_lasso_fort <- loo(matrix(unlist(summaries$log_like),
                                 n_samps / n_thin, sum(nobs), byrow=FALSE))
rm(samps, summaries, betas, s_invs)
  save(y_tilde_mean_pca_lasso_fort, y_tilde_sd_pca_lasso_fort, loo_pca_lasso_fort, 
       file="./observer-fit/pcalasso.RData")
}
```

```{r tPCASSVSObserver, echo=FALSE, eval=TRUE}
if (file.exists("./observer-fit/tpcassvs.RData")) {
  load("./observer-fit/tpcassvs.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(t, 0, 100))
}

dat <- list(Y=Y_sparse, X=X_PCA$X_pca[, 1:p], p=p, D=c(X_PCA$sdev^2),
            H=H_sparse, t=t, tt=tt, N_total=sum(nobs))

params <- c("gamma", "beta", "s_inv", "tau2_beta", "nu", "v_inv", "log_like")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/tPCASSVS.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="tpca_ssvs", type="observer")


samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

nus <- matrix(0, n_samps, t)
v_invs <- matrix(0, n_samps, sum(nobs))
s_invs <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
log_like <- matrix(0, n_samps, sum(nobs))
for(i in 1:t){
  s_invs[, i] <- samps[, paste("s_inv[", i, "]", sep="")]
  nus[, i] <- samps[, paste("nu[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}
for(i in 1:sum(nobs)){
  log_like[, i] <- log(samps[, paste("log_like[", i, "]", sep="")])
  v_invs[, i] <- samps[, paste("v_inv[", i, "]", sep="")]
}

summaries <- processPCASamplesT2(t, p, n_samps, N, betas, s_invs, nus, v_invs,
                                 X_PCA$X_pca[, 1:p], Y.list, H.list, mu, nobs,
                                 n_thin, preds=FALSE)

y_tilde_mean_tpca_ssvs_fort <- summaries$y_tilde_mean
y_tilde_sd_tpca_ssvs_fort <- summaries$y_tilde_sd
loo_tpca_ssvs_fort <- loo(log_like)
rm(samps, nus, v_invs, s_invs, betas, log_like, summaries)
  save(y_tilde_mean_tpca_ssvs_fort, y_tilde_sd_tpca_ssvs_fort, loo_tpca_ssvs_fort, 
       file="./observer-fit/tpcassvs.RData")
}
```


```{r tPCALassoObserver, echo=FALSE, eval=TRUE}
if (file.exists("./observer-fit/tpcalasso.RData")) {
  load("./observer-fit/tpcalasso.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(t, 0, 100),
       gamma2=matrix(rgamma(p*t, 1, 1), p, t), lambda2 = rexp(t))
}

dat <- list(Y=Y_sparse, X=X_PCA$X_pca[, 1:p], p=p, H=H_sparse, t=t,
            tt=tt, N_total=sum(nobs), D=(X_PCA$sdev^2)[1:p])

params <- c("gamma2", "lambda2", "beta", "v_inv", "nu", "s_inv", "log_like")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/tPCALasso.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="tpca_lasso", type="observer")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

nus <- matrix(0, n_samps, t)
v_invs <- matrix(0, n_samps, sum(nobs))
s_invs <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
log_like <- matrix(0, n_samps, sum(nobs))
for(i in 1:t){
  s_invs[, i] <- samps[, paste("s_inv[", i, "]", sep="")]
  nus[, i] <- samps[, paste("nu[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}
for(i in 1:sum(nobs)){
  log_like[, i] <- log(samps[, paste("log_like[", i, "]", sep="")])
  v_invs[, i] <- samps[, paste("v_inv[", i, "]", sep="")]
}

summaries <- processPCASamplesT2(t, p, n_samps, N, betas, s_invs, nus, v_invs,
                                 X_PCA$X_pca[, 1:p], Y.list, H.list, mu, nobs,
                                 n_thin, preds=FALSE)

y_tilde_mean_tpca_lasso_fort <- summaries$y_tilde_mean
y_tilde_sd_tpca_lasso_fort <- summaries$y_tilde_sd
loo_tpca_lasso_fort <- loo(log_like)
rm(samps, nus, v_invs, s_invs, betas, log_like, summaries)
  save(y_tilde_mean_tpca_lasso_fort, y_tilde_sd_tpca_lasso_fort,
       loo_tpca_lasso_fort, file="./observer-fit/tpcalasso.RData")
}
```

```{r pPCASSVSObserver, echo=FALSE, eval=TRUE}
if (file.exists("./observer-fit/ppcassvs.RData")) {
  load("./observer-fit/ppcassvs.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(s_beta_inv=runif(1, 0, 10), beta=matrix(rnorm(p, 0, 10), p, t),
       tau_inv=runif(t, 0, 100), s_inv=runif(1, 0, 10))
}

dat <- list(Y=Y_sparse, X=X_PPCA$X_center, H=H_sparse, p=p, K_hat=X_PPCA$K_hat,
            tKK=X_PPCA$Lambda, I_p=diag(p), t=t, tt=tt, N_total=sum(nobs))

params <- c("gamma", "beta", "tau2", "s2", "tau2_beta")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/pPCASSVS.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="ppca_ssvs", type="observer")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

tau2s <- matrix(0, n_samps, t)
s2s <- c(samps[, "s2"])
betas <- array(0, dim=c(n_samps, t, p))
for(i in 1:t){
  tau2s[, i] <- samps[, paste("tau2[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}

summaries <- processSamples(t, p, n_samps, N, betas, tau2s, s2s, Y.list, H.list,
                            nobs, X_PPCA$X_center, X_PPCA$Lambda, diag(p),
                            X_PPCA$K_hat, mu, n_thin, preds=FALSE)

y_tilde_mean_ppca_ssvs_fort <- summaries$y_tilde_mean
y_tilde_sd_ppca_ssvs_fort <- summaries$y_tilde_sd
loo_ppca_ssvs_fort <- loo(matrix(unlist(summaries$log_like),
                                 n_samps / n_thin, sum(nobs), byrow=FALSE))
rm(samps, summaries, betas, s2s, tau2s)
  save(y_tilde_mean_ppca_ssvs_fort, y_tilde_sd_ppca_ssvs_fort, 
       loo_ppca_ssvs_fort, file="./observer-fit/ppcassvs.RData")
}
```

```{r pPCALassoObserver, echo=FALSE, eval=TRUE}
if (file.exists("./observer-fit/ppcalasso.RData")) {
  load("./observer-fit/ppcalasso.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(beta=matrix(rnorm(t*p, 0, 10), p, t), tau_inv=runif(t, 0, 100),
       s_inv=runif(1, 0, 10))
}

dat <- list(Y=Y_sparse, X=X_PPCA$X_center, H=H_sparse, p=p, K_hat=X_PPCA$K_hat,
            tKK=X_PPCA$Lambda, I_p=diag(p), t=t, tt=tt, N_total=sum(nobs))

params <- c("gamma2", "lambda2", "beta", "tau2", "s2")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/pPCALasso.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="ppca_lasso", type="observer")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

tau2s <- matrix(0, n_samps, t)
s2s <- c(samps[, "s2"])
betas <- array(0, dim=c(n_samps, t, p))
for(i in 1:t){
  tau2s[, i] <- samps[, paste("tau2[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}

summaries <- processSamples(t, p, n_samps, N, betas, tau2s, s2s, Y.list, H.list,
                            nobs, X_PPCA$X_center, X_PPCA$Lambda, diag(p),
                            X_PPCA$K_hat, mu, n_thin, preds=FALSE)

y_tilde_mean_ppca_lasso_fort <- summaries$y_tilde_mean
y_tilde_sd_ppca_lasso_fort <- summaries$y_tilde_sd
loo_ppca_lasso_fort <- loo(matrix(unlist(summaries$log_like),
                                  n_samps / n_thin, sum(nobs), byrow=FALSE))
rm(samps, summaries, betas, s2s, tau2s)
  save(y_tilde_mean_ppca_lasso_fort, y_tilde_sd_ppca_lasso_fort,
       loo_ppca_lasso_fort, file="./observer-fit/ppcalasso.RData")
}
```

```{r tpPCASSVSObserver, echo=FALSE, eval=TRUE}
if (file.exists("./observer-fit/tppcassvs.RData")) {
  load("./observer-fit/tppcassvs.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(1, 0, 10),
       tau_inv=runif(t, 0, 100)) }

dat <- list(Y=Y_sparse, X=X_PPCA$X_center, H=H_sparse, p=p, K_hat=X_PPCA$K_hat,
            tKK=X_PPCA$Lambda, I_p=diag(p), t=t, tt=tt, N_total=sum(nobs))

params <- c("gamma", "beta", "s2", "tau2_beta", "nu", "v_inv", "log_like",
            "tau2")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/tpPCASSVS.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostics
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="tppca_ssvs", type="observer")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

nus <- matrix(0, n_samps, t)
v_invs <- matrix(0, n_samps, sum(nobs))
s2s <- samps[, paste("s2")]
tau2s <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
log_like <- matrix(0, n_samps, sum(nobs))
for(i in 1:t){
  tau2s[, i] <- samps[, paste("tau2[", i, "]", sep="")]
  nus[, i] <- samps[, paste("nu[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}
for(i in 1:sum(nobs)){
  log_like[, i] <- log(samps[, paste("log_like[", i, "]", sep="")])
  v_invs[, i] <- samps[, paste("v_inv[", i, "]", sep="")]
}

summaries <- processSamplesT(t, p, n_samps, N, betas, tau2s, s2s, nus,
                             v_invs, Y.list, H.list, nobs, X_PPCA$X_center,
                             X_PPCA$Lambda, diag(p), X_PPCA$K_hat, mu, n_thin,
                             preds=FALSE)

y_tilde_mean_tppca_ssvs_fort <- summaries$y_tilde_mean
y_tilde_sd_tppca_ssvs_fort <- summaries$y_tilde_sd
loo_tppca_ssvs_fort <- loo(log_like)
rm(samps, nus, v_invs, s2s, tau2s, betas, log_like, summaries)
  save(y_tilde_mean_tppca_ssvs_fort, y_tilde_sd_tppca_ssvs_fort, 
       loo_tppca_ssvs_fort, file="./observer-fit/tppcassvs.RData")
}
```

```{r tpPCALassoObserver, echo=FALSE, eval=TRUE}
if (file.exists("./observer-fit/tppcalasso.RData")) {
  load("./observer-fit/tppcalasso.RData")
} else {
  ## fit mcmc
  
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t),  s_inv=runif(1, 0, 10),
       tau_inv=runif(t, 0, 100), gamma2=matrix(rgamma(p*t, 1, 1), p, t),
       lambda2=rexp(t))
}

dat <- list(Y=Y_sparse, X=X_PPCA$X_center, H=H_sparse, p=p, K_hat=X_PPCA$K_hat,
            tKK=X_PPCA$Lambda, I_p=diag(p), t=t, tt=tt, N_total=sum(nobs))

params <- c("gamma2", "lambda2", "beta", "nu", "s2", "tau2", "v_inv",
            "log_like")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/tpPCALasso.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostics
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="tppca_lasso", type="observer")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

nus <- matrix(0, n_samps, t)
v_invs <- matrix(0, n_samps, sum(nobs))
s2s <- samps[, paste("s2")]
tau2s <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
log_like <- matrix(0, n_samps, sum(nobs))
for(i in 1:t){
  tau2s[, i] <- samps[, paste("tau2[", i, "]", sep="")]
  nus[, i] <- samps[, paste("nu[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}
for(i in 1:sum(nobs)){
  log_like[, i] <- log(samps[, paste("log_like[", i, "]", sep="")])
  v_invs[, i] <- samps[, paste("v_inv[", i, "]", sep="")]
}

summaries <- processSamplesT(t, p, n_samps, N, betas, tau2s, s2s, nus, v_invs,
                             Y.list, H.list, nobs, X_PPCA$X_center,
                             X_PPCA$Lambda, diag(p), X_PPCA$K_hat, mu, n_thin,
                             preds=FALSE)

y_tilde_mean_tppca_lasso_fort <- summaries$y_tilde_mean
y_tilde_sd_tppca_lasso_fort <- summaries$y_tilde_sd
loo_tppca_lasso_fort <- loo(log_like)
rm(samps, nus, v_invs, s2s, tau2s, betas, log_like, summaries)
  save(y_tilde_mean_tppca_lasso_fort, y_tilde_sd_tppca_lasso_fort,
       loo_tppca_lasso_fort, file="./observer-fit/tppcalasso.RData")
}
```

## Process Observer data predictions

```{r simTablesFort, echo=FALSE, message=FALSE, eval=TRUE}
loo_preds_fort <- data.frame(looic= NA)
loo_ic_fort <- data.frame(cbind(c(loo_pca_ssvs_fort$looic,
                                  loo_pca_lasso_fort$looic),
                                c(loo_ppca_ssvs_fort$looic,
                                  loo_ppca_lasso_fort$looic),
                                c(loo_tpca_ssvs_fort$looic,
                                  loo_tpca_lasso_fort$looic),
                                c(loo_tppca_ssvs_fort$looic,
                                  loo_tppca_lasso_fort$looic)))
rownames(loo_ic_fort) <- c("SSVS", "LASSO")
colnames(loo_ic_fort) <- c("PCR", "pPCR", "PCR", "pPCR")
addtorow$pos <- list(0)
addtorow$command <- c("\\multicolumn{1}{|r|}{} & \\multicolumn{2}{c|}{Gaussian} & \\multicolumn{2}{c|}{Robust} \\\\
& PCR & pPCR & PCR & pPCR \\\\\n")
print(xtable(loo_ic_fort, digits=0, align=c("|r|rr|rr|")),
      hline.after=c(-1, 0, nrow(loo_ic_fort)), add.to.row=addtorow,
      file="loofort.tex", floating=FALSE, size="small", include.colnames=FALSE)
```

\begin{table}
\centering
\input{./loofort}
\caption{Fort historical reconstruction scores. Smaller values indicate better model performance}
\end{table}

<!-- \begin{table} -->
<!-- \centering -->
<!-- \input{./fort-scores} -->
<!-- \caption{Fort historical reconstruction scores. Smaller values indicate better model performance} -->
<!-- \label{tab:fort} -->
<!-- \end{table} -->

```{r, cache=TRUE}
finish_observer <- Sys.time()
observer_time <- finish_observer - start_observer
observer_time
```

## Remove outlier from data

```{r fortDataRemoveOutlier, echo=FALSE, eval=TRUE}
# data(fortFunctions)

mu_outlier <- mu
nobs_outlier <- nobs
outlier_idx <- which(loo_pca_ssvs_fort$pareto_k == max(loo_pca_ssvs_fort$pareto_k))
nobs_outlier[tt[outlier_idx]]<- nobs_outlier[tt[outlier_idx]] - 1
Y.list.outlier <- Y.list
Y_sparse_outlier <- Y_sparse
H.list.outlier <- H.list
tt_outlier <- tt[-outlier_idx]


Y.list.outlier[[tt[outlier_idx]]] <-
  Y.list.outlier[[tt[outlier_idx]]][-which(Y.list.outlier[[tt[outlier_idx]]] ==
                                             min(Y.list.outlier[[tt[outlier_idx]]]))]
mu_outlier[tt[outlier_idx]] <- mean(Y.list.outlier[[tt[outlier_idx]]])
H.list.outlier[[tt[outlier_idx]]] <-
  H.list.outlier[[tt[outlier_idx]]][-which(Y.list.outlier[[tt[outlier_idx]]] ==
                                             min(Y.list.outlier[[tt[outlier_idx]]]))]
Y_sparse_outlier <- Y_sparse[-outlier_idx]
H_sparse_outlier <- H_sparse[-outlier_idx]
```

## Re-fit candidate models

```{r, cache=TRUE}
start_outlier <- Sys.time()
```

```{r PCASSVSOutlier, echo=FALSE, eval=TRUE}
if (file.exists("./outlier-fit/pcassvs.RData")) {
  load("./outlier-fit/pcassvs.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(t, 0, 100))
}

dat <- list(Y=Y_sparse_outlier, X=X_PCA$X_pca[, 1:p], p=p, D=c(X_PCA$sdev^2),
            H=H_sparse_outlier, t=t, tt=tt_outlier, N_total=length(tt_outlier))

params <- c("gamma", "beta", "s_inv", "tau2_beta")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/PCASSVS.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="pca_ssvs", type = "outlier-removed")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

s_invs <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
for(i in 1:t){
  s_invs[, i] <- samps[, paste("s_inv[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}

summaries <- processPCASamples(t, p, n_samps, N, betas, s_invs,
                               X_PCA$X_pca[, 1:p], Y.list.outlier,
                               H.list.outlier,  mu_outlier, nobs_outlier, n_thin,
                               preds=FALSE)

y_tilde_mean_pca_ssvs_outlier <- summaries$y_tilde_mean
y_tilde_sd_pca_ssvs_outlier <- summaries$y_tilde_sd
loo_pca_ssvs_outlier <- loo(matrix(unlist(summaries$log_like),
                                        n_samps / n_thin, sum(nobs_outlier), byrow=FALSE))
rm(samps, summaries, betas, s_invs)
  save(y_tilde_mean_pca_ssvs_outlier, y_tilde_sd_pca_ssvs_outlier,
       loo_pca_ssvs_outlier, file="./outlier-fit/pcassvs.RData")
}
```

```{r PCALassoOutlier, eval=TRUE, echo=FALSE, eval=TRUE}
if (file.exists("./outlier-fit/pcalasso.RData")) {
  load("./outlier-fit/pcalasso.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(t, 0, 100),
       gamma2=matrix(rgamma(p*t, 1, 1), p, t), lambda2 = rexp(t))
}

dat <- list(Y=Y_sparse_outlier, X=X_PCA$X_pca[, 1:p], p=p, H=H_sparse_outlier,
            t=t, tt=tt_outlier, N_total=length(tt_outlier), D=c(X_PCA$sdev^2))

params <- c("beta", "s_inv", "gamma2", "lambda2")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/PCALasso.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="pca_lasso", type="outlier-removed")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

s_invs <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
for(i in 1:t){
  s_invs[, i] <- samps[, paste("s_inv[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}

summaries <- processPCASamples(t, p, n_samps, N, betas, s_invs,
                               X_PCA$X_pca[, 1:p], Y.list.outlier,
                               H.list.outlier, mu_outlier, nobs_outlier, n_thin,
                               preds=FALSE)

y_tilde_mean_pca_lasso_outlier <- summaries$y_tilde_mean
y_tilde_sd_pca_lasso_outlier <- summaries$y_tilde_sd
loo_pca_lasso_outlier <- loo(matrix(unlist(summaries$log_like),
                                         n_samps / n_thin, sum(nobs_outlier), byrow=FALSE))
rm(samps, summaries, betas, s_invs)
  save(y_tilde_mean_pca_lasso_outlier, y_tilde_sd_pca_lasso_outlier,
       loo_pca_lasso_outlier, file="./outlier-fit/pcalasso.RData")
}

```

```{r tPCASSVSOutlier, echo=FALSE, eval=TRUE}
if (file.exists("./outlier-fit/tpcassvs.RData")) {
  load("./outlier-fit/tpcassvs.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(t, 0, 100))
}

dat <- list(Y=Y_sparse_outlier, X=X_PCA$X_pca[, 1:p], p=p, D=c(X_PCA$sdev^2),
            H=H_sparse_outlier, t=t, tt=tt_outlier, N_total=length(tt_outlier))

params <- c("gamma", "beta", "s_inv", "tau2_beta", "nu", "v_inv", "log_like")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/tPCASSVS.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="tpca_ssvs", type="outlier-removed")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

nus <- matrix(0, n_samps, t)
v_invs <- matrix(0, n_samps, sum(nobs_outlier))
s_invs <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
log_like <- matrix(0, n_samps, sum(nobs_outlier))
for(i in 1:t){
  s_invs[, i] <- samps[, paste("s_inv[", i, "]", sep="")]
  nus[, i] <- samps[, paste("nu[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}
for(i in 1:sum(nobs_outlier)){
  log_like[, i] <- log(samps[, paste("log_like[", i, "]", sep="")])
  v_invs[, i] <- samps[, paste("v_inv[", i, "]", sep="")]
}

summaries <- processPCASamplesT2(t, p, n_samps, N, betas, s_invs, nus, v_invs,
                                 X_PCA$X_pca[, 1:p], Y.list.outlier,
                                 H.list.outlier, mu_outlier, nobs_outlier, n_thin,
                                 preds=FALSE)

y_tilde_mean_tpca_ssvs_outlier <- summaries$y_tilde_mean
y_tilde_sd_tpca_ssvs_outlier <- summaries$y_tilde_sd
loo_tpca_ssvs_outlier <- loo(log_like)
rm(samps, nus, v_invs, s_invs, betas, log_like, summaries)
  save(y_tilde_mean_tpca_ssvs_outlier, y_tilde_sd_tpca_ssvs_outlier,
       loo_tpca_ssvs_outlier, file="./outlier-fit/tpcassvs.RData")
}

```

```{r tPCALassoOutlier, echo=FALSE, eval=TRUE}
if (file.exists("./outlier-fit/tpcalasso.RData")) {
  load("./outlier-fit/tpcalasso.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(t, 0, 100),
       gamma2=matrix(rgamma(p*t, 1, 1), p, t), lambda2 = rexp(t))
}

dat <- list(Y=Y_sparse_outlier, X=X_PCA$X_pca[, 1:p], p=p, H=H_sparse_outlier, t=t,
            tt=tt_outlier, N_total=length(tt_outlier), D=(X_PCA$sdev^2)[1:p])

params <- c("gamma2", "lambda2", "beta", "v_inv", "nu", "s_inv", "log_like")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/tPCALasso.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="tpca_lasso", type="outlier-removed")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

nus <- matrix(0, n_samps, t)
v_invs <- matrix(0, n_samps, sum(nobs_outlier))
s_invs <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
log_like <- matrix(0, n_samps, sum(nobs_outlier))
for(i in 1:t){
  s_invs[, i] <- samps[, paste("s_inv[", i, "]", sep="")]
  nus[, i] <- samps[, paste("nu[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}
for(i in 1:sum(nobs_outlier)){
  log_like[, i] <- log(samps[, paste("log_like[", i, "]", sep="")])
  v_invs[, i] <- samps[, paste("v_inv[", i, "]", sep="")]
}

summaries <- processPCASamplesT2(t, p, n_samps, N, betas, s_invs, nus, v_invs,
                                 X_PCA$X_pca[, 1:p], Y.list.outlier, H.list.outlier, mu_outlier, nobs_outlier,
                                 n_thin, preds=FALSE)

y_tilde_mean_tpca_lasso_outlier <- summaries$y_tilde_mean
y_tilde_sd_tpca_lasso_outlier <- summaries$y_tilde_sd
loo_tpca_lasso_outlier <- loo(log_like)
rm(samps, nus, v_invs, s_invs, betas, log_like, summaries)
  save(y_tilde_mean_tpca_lasso_outlier, y_tilde_sd_tpca_lasso_outlier,
       loo_tpca_lasso_outlier, file="./outlier-fit/tpcalasso.RData")
}

```

```{r pPCASSVSOutlier, echo=FALSE, eval=TRUE}
if (file.exists("./outlier-fit/ppcassvs.RData")) {
  load("./outlier-fit/ppcassvs.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(s_beta_inv=runif(1, 0, 10), beta=matrix(rnorm(p, 0, 10), p, t),
       tau_inv=runif(t, 0, 100), s_inv=runif(1, 0, 10))
}

dat <- list(Y=Y_sparse_outlier, X=X_PPCA$X_center, H=H_sparse_outlier, p=p,
            K_hat=X_PPCA$K_hat, tKK=X_PPCA$Lambda, I_p=diag(p), t=t,
            tt=tt_outlier, N_total=length(tt_outlier))

params <- c("gamma", "beta", "tau2", "s2", "tau2_beta")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/pPCASSVS.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostics
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="ppca_ssvs", type="outlier-removed")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

tau2s <- matrix(0, n_samps, t)
s2s <- c(samps[, "s2"])
betas <- array(0, dim=c(n_samps, t, p))
for(i in 1:t){
  tau2s[, i] <- samps[, paste("tau2[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}

summaries <- processSamples(t, p, n_samps, N, betas, tau2s, s2s,
                            Y.list.outlier, H.list.outlier, nobs_outlier,
                            X_PPCA$X_center, X_PPCA$Lambda, diag(p),
                            X_PPCA$K_hat, mu_outlier, n_thin, preds=FALSE)

y_tilde_mean_ppca_ssvs_outlier <- summaries$y_tilde_mean
y_tilde_sd_ppca_ssvs_outlier <- summaries$y_tilde_sd
loo_ppca_ssvs_outlier <- loo(matrix(unlist(summaries$log_like),
                                         n_samps / n_thin, sum(nobs_outlier), byrow=FALSE))
rm(samps, summaries, betas, s2s, tau2s)
  save(y_tilde_mean_ppca_ssvs_outlier, y_tilde_sd_ppca_ssvs_outlier,
       loo_ppca_ssvs_outlier, file="./outlier-fit/ppcassvs.RData")
}

```

```{r pPCALassoOutlier, echo=FALSE, eval=TRUE}
if (file.exists("./outlier-fit/ppcalasso.RData")) {
  load("./outlier-fit/ppcalasso.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(beta=matrix(rnorm(t*p, 0, 10), p, t), tau_inv=runif(t, 0, 100),
       s_inv=runif(1, 0, 10))
}

dat <- list(Y=Y_sparse_outlier, X=X_PPCA$X_center, H=H_sparse_outlier, p=p,
            K_hat=X_PPCA$K_hat, tKK=X_PPCA$Lambda, I_p=diag(p), t=t,
            tt=tt_outlier, N_total=length(tt_outlier))

params <- c("gamma2", "lambda2", "beta", "tau2", "s2")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/pPCALasso.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostic Checks
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="ppca_lasso", type="outlier-removed")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

tau2s <- matrix(0, n_samps, t)
s2s <- c(samps[, "s2"])
betas <- array(0, dim=c(n_samps, t, p))
for(i in 1:t){
  tau2s[, i] <- samps[, paste("tau2[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}

summaries <- processSamples(t, p, n_samps, N, betas, tau2s, s2s,
                            Y.list.outlier, H.list.outlier, nobs_outlier,
                            X_PPCA$X_center, X_PPCA$Lambda, diag(p),
                            X_PPCA$K_hat, mu_outlier, n_thin, preds=FALSE)

y_tilde_mean_ppca_lasso_outlier <- summaries$y_tilde_mean
y_tilde_sd_ppca_lasso_outlier <- summaries$y_tilde_sd
loo_ppca_lasso_outlier <- loo(matrix(unlist(summaries$log_like),
                                          n_samps / n_thin, sum(nobs_outlier), byrow=FALSE))
rm(samps, summaries, betas, s2s, tau2s)
  save(y_tilde_mean_ppca_lasso_outlier, y_tilde_sd_ppca_lasso_outlier,
       loo_ppca_lasso_outlier, file="./outlier-fit/ppcalasso.RData")
}

```

```{r tpPCASSVSOutlier, echo=FALSE, eval=TRUE}
if (file.exists("./outlier-fit/tppcassvs.RData")) {
  load("./outlier-fit/tppcassvs.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t), s_inv=runif(1, 0, 10),
       tau_inv=runif(t, 0, 100)) }

dat <- list(Y=Y_sparse_outlier, X=X_PPCA$X_center, H=H_sparse_outlier, p=p,
            K_hat=X_PPCA$K_hat, tKK=X_PPCA$Lambda, I_p=diag(p), t=t,
            tt=tt_outlier, N_total=length(tt_outlier))

params <- c("gamma", "beta", "s2", "tau2_beta", "nu", "v_inv", "log_like",
            "tau2")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/tpPCASSVS.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostics
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="tppca_ssvs", type="outlier-removed")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

nus <- matrix(0, n_samps, t)
v_invs <- matrix(0, n_samps, sum(nobs_outlier))
s2s <- samps[, paste("s2")]
tau2s <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
log_like <- matrix(0, n_samps, sum(nobs_outlier))
for(i in 1:t){
  tau2s[, i] <- samps[, paste("tau2[", i, "]", sep="")]
  nus[, i] <- samps[, paste("nu[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}
for(i in 1:sum(nobs_outlier)){
  log_like[, i] <- log(samps[, paste("log_like[", i, "]", sep="")])
  v_invs[, i] <- samps[, paste("v_inv[", i, "]", sep="")]
}

summaries <- processSamplesT(t, p, n_samps, N, betas, tau2s, s2s, nus,
                             v_invs, Y.list.outlier, H.list.outlier, nobs_outlier,
                             X_PPCA$X_center, X_PPCA$Lambda, diag(p),
                             X_PPCA$K_hat, mu_outlier, n_thin, preds=FALSE)

y_tilde_mean_tppca_ssvs_outlier <- summaries$y_tilde_mean
y_tilde_sd_tppca_ssvs_outlier <- summaries$y_tilde_sd
loo_tppca_ssvs_outlier <- loo(log_like)
rm(samps, nus, v_invs, s2s, tau2s, betas, log_like, summaries)
  save(y_tilde_mean_tppca_ssvs_outlier, y_tilde_sd_tppca_ssvs_outlier,
       loo_tppca_ssvs_outlier, file="./outlier-fit/tppcassvs.RData")
}

```

```{r tpPCALassoOutlier, echo=FALSE, eval=TRUE}
if (file.exists("./outlier-fit/tppcalasso.RData")) {
  load("./outlier-fit/tppcalasso.RData")
} else {
  ## fit mcmc
  jinits <- function(){
  list(beta=matrix(rnorm(p, 0, 10), p, t),  s_inv=runif(1, 0, 10),
       tau_inv=runif(t, 0, 100), gamma2=matrix(rgamma(p*t, 1, 1), p, t),
       lambda2=rexp(t))
}

dat <- list(Y=Y_sparse_outlier, X=X_PPCA$X_center, H=H_sparse_outlier, p=p,
            K_hat=X_PPCA$K_hat, tKK=X_PPCA$Lambda, I_p=diag(p), t=t,
            tt=tt_outlier, N_total=length(tt_outlier))

params <- c("gamma2", "lambda2", "beta", "nu", "s2", "tau2", "v_inv",
            "log_like")

samps <- foreach(i=1:n_chains, .inorder = FALSE,
                 .packages = c("rjags", "random"), .combine = "mcmc.combine",
                 .multicombine = TRUE) %dopar% {
                   model.jags <- jags.model("./models-sens/tpPCALasso.txt",
                                            data=dat, inits=jinits, n.chain=1, n.adapt=n_adapt)
                   update(model.jags, n_burn)
                   result <- coda.samples(model.jags, params, n_iter)
                   return(result)
                 }

## Diagnostics
make_diagnostics(samps, t=t, p=p, nobs=nobs,  model="tppca_lasso", type="outlier-removed")

samps <- combine.mcmc(samps)
n_samps <- dim(samps)[1]

nus <- matrix(0, n_samps, t)
v_invs <- matrix(0, n_samps, sum(nobs_outlier))
s2s <- samps[, paste("s2")]
tau2s <- matrix(0, n_samps, t)
betas <- array(0, dim=c(n_samps, t, p))
log_like <- matrix(0, n_samps, sum(nobs_outlier))
for(i in 1:t){
  tau2s[, i] <- samps[, paste("tau2[", i, "]", sep="")]
  nus[, i] <- samps[, paste("nu[", i, "]", sep="")]
  for(j in 1:p){
    betas[, i, j] <- samps[, paste("beta[", j, ",", i, "]", sep="")]
  }
}
for(i in 1:sum(nobs_outlier)){
  log_like[, i] <- log(samps[, paste("log_like[", i, "]", sep="")])
  v_invs[, i] <- samps[, paste("v_inv[", i, "]", sep="")]
}

summaries <- processSamplesT(t, p, n_samps, N, betas, tau2s, s2s, nus, v_invs,
                             Y.list.outlier, H.list.outlier, nobs_outlier,
                             X_PPCA$X_center, X_PPCA$Lambda, diag(p),
                             X_PPCA$K_hat, mu_outlier, n_thin, preds=FALSE)

y_tilde_mean_tppca_lasso_outlier <- summaries$y_tilde_mean
y_tilde_sd_tppca_lasso_outlier <- summaries$y_tilde_sd
loo_tppca_lasso_outlier <- loo(log_like)
rm(samps, nus, v_invs, s2s, tau2s, betas, log_like, summaries)
  save(y_tilde_mean_tppca_lasso_outlier, y_tilde_sd_tppca_lasso_outlier,
       loo_tppca_lasso_outlier, file="./outlier-fit/tppcalasso.RData")
}

```

```{r, cache=TRUE}
finish_outlier <- Sys.time()
outlier_time <- finish_outlier - start_outlier
outlier_time
```
## Check for residual spatial autocorrelation

```{r variogramPCA, cache=TRUE, echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.width=6, fig.keep='all', fig.show='hide', eval=TRUE, results='hide'}
# data(fortFunctions)

Y_obs <- rep(0, sum(nobs))
Y_hat <- rep(0, sum(nobs))
for (i in 1:sum(nobs)) {
  Y_obs[i] <- Y_sparse[i] + mu[tt[i]]
  Y_hat[i] <- y_tilde_mean_tpca_lasso_outlier[H_sparse[i], tt[i]]
}
outlier_idx <- which(Y_obs == min(Y_obs))
Y_obs <- Y_obs[ - outlier_idx]
Y_hat <- Y_hat[ - outlier_idx]
resids <- Y_obs - Y_hat


layout(matrix(1:3, 3, 1))
hist(Y_obs)
hist(Y_hat)
hist(resids)
# min(resids)

# dists3 <- dist(as.matrix(locs[H_sparse, ]))
# dists <- distGeo(as.matrix(locs[H_sparse, ]), as.matrix(locs[H_sparse, ]))
dists <- fields::rdist.earth(as.matrix(locs[H_sparse, ]), as.matrix(locs[H_sparse, ]))
# colnames(locs) <- c("X", "Y")
locs.df <- data.frame(ID=1:length(locs[, 1]), X=locs[, 1], Y=locs[, 2])
coordinates(locs.df) <- c("X", "Y")
proj4string(locs.df) <- CRS("+proj=longlat +datum=WGS84")  ## for example
utms <- spTransform(locs.df, CRS(paste("+proj=utm +zone=",15," ellps=WGS84",sep='')))

num_breaks <- 100
dists <- dist(utms@coords[H_sparse, ][ - outlier_idx, ])
breaks <- seq(min(dists), max(dists), length=num_breaks)
breaksstar <- seq(0, 50000, length=30)
v0 <- variog(coords=utms@coords[H_sparse, ][ - outlier_idx, ], data=resids,
             breaks=breaks, messages=FALSE)
vstar <- variog(coords=utms@coords[H_sparse, ][ - outlier_idx, ], data=resids,
                breaks=breaksstar, messages=FALSE)
vtime <- variog(coords=utms@coords[H_sparse, ][tt == 56, ], data=resids[tt == 56],
                messages=FALSE)

# v1 <- variog(coords=utms@coords[H_sparse, ][ - outlier_idx,], data=resids,
#              option="cloud", messages=FALSE)
# v2 <- variog(coords=utms@coords[H_sparse, ][ - outlier_idx, ], data=resids,
#              option="smooth", messages=FALSE)
# v0.summary <- cbind(1:num_breaks, v0$v, v0$n)
# colnames(v0.summary) <- c("lag", "semi-variance", "# of pairs")
# v0.summary
layout(matrix(1:2, 2, 1))
plot(v0, type = "b", main = "Variogram: robust PCA LASSO")
plot(vstar, type = "b", main = "Variogram: robust PCA LASSO small scale")
plot(vtime, type = "b", main = "Variogram: robust PCA LASSO one year")
# plot(v1, type = "b", main = "Variogram: robust PCA LASSO", bin.cloud=FALSE)
# plot(v2, type = "b", main = "Variogram: robust PCA LASSO")
# lines(v1)
# points(v1$u, v1$v, type="p", col=adjust.color('black', alpha.f=0.025))
# lines(v2)
```

```{r variogramPPCA, cache=TRUE, echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.width=6, fig.keep='all', fig.show='hide', eval=TRUE, results='hide'}
# data(fortFunctions)

Y_obs <- rep(0, sum(nobs))
Y_hat <- rep(0, sum(nobs))
for (i in 1:sum(nobs)) {
  Y_obs[i] <- Y_sparse[i] + mu[tt[i]]
  Y_hat[i] <- y_tilde_mean_tppca_lasso_outlier[H_sparse[i], tt[i]]
}
outlier_idx <- which(Y_obs == min(Y_obs))
Y_obs <- Y_obs[ - outlier_idx]
Y_hat <- Y_hat[ - outlier_idx]
resids <- Y_obs - Y_hat

layout(matrix(1:3, 3, 1))
hist(Y_obs)
hist(Y_hat)
hist(resids)

# dists3 <- dist(as.matrix(locs[H_sparse, ]))
# dists <- distGeo(as.matrix(locs[H_sparse, ]), as.matrix(locs[H_sparse, ]))
dists <- fields::rdist.earth(as.matrix(locs[H_sparse, ]), as.matrix(locs[H_sparse, ]))
# colnames(locs) <- c("X", "Y")
locs.df <- data.frame(ID=1:length(locs[, 1]), X=locs[, 1], Y=locs[, 2])
coordinates(locs.df) <- c("X", "Y")
proj4string(locs.df) <- CRS("+proj=longlat +datum=WGS84")  ## for example
utms <- spTransform(locs.df, CRS(paste("+proj=utm +zone=",15," ellps=WGS84",sep='')))

num_breaks <- 100
dists <- dist(utms@coords[H_sparse, ][ - outlier_idx, ])
breaks <- seq(min(dists), max(dists), length=num_breaks)
breaksstar <- seq(0, 50000, length=30)
v0 <- variog(coords=utms@coords[H_sparse, ][ - outlier_idx, ], data=resids,
             breaks=breaks, messages=FALSE)
vstar <- variog(coords=utms@coords[H_sparse, ][ - outlier_idx, ], data=resids,
                breaks=breaksstar, messages=FALSE)
# v1 <- variog(coords=utms@coords[H_sparse, ][ - outlier_idx,], data=resids,
#              option="cloud", messages=FALSE)
# v2 <- variog(coords=utms@coords[H_sparse, ][ - outlier_idx, ], data=resids,
#              option="smooth", messages=FALSE)
# v0.summary <- cbind(1:num_breaks, v0$v, v0$n)
# colnames(v0.summary) <- c("lag", "semi-variance", "# of pairs")
# v0.summary
layout(matrix(1:2, 2, 1))
plot(v0, type = "b", main = "Variogram: robust PPCA LASSO")
plot(vstar, type = "b", main = "Variogram: robust PPCA LASSO small scale")
# plot(v1, type = "b", main = "Variogram: robust PCA LASSO", bin.cloud=FALSE)
# plot(v2, type = "b", main = "Variogram: robust PCA LASSO")
# lines(v1)
# points(v1$u, v1$v, type="p", col=adjust.color('black', alpha.f=0.025))
# lines(v2)
```

## Leave-one-out approximations

```{r loo_fort, echo=FALSE, message=FALSE, include=FALSE, warning=FALSE, fig.height=6, fig.width=12, eval=TRUE}
## PCR
par(mfrow=c(1, 2), mar=c(3.5, 3.5, 1.75, 1) + 0.1, oma=c(0, 0, 0.25, 0.25) + 0.1, mgp=c(2, 1, 0))
plot_k(loo_pca_lasso_fort$pareto_k, main="LASSO PCR reconstruction", ylim=c(0, 4.0),
       cex.axis=1.5, cex.main=1.5, cex.lab=1.5)
mtext("(a)",side=3,line=-1.9,adj=c(0.1),cex=2,col="black")
plot_k(loo_pca_lasso_outlier$pareto_k, main="LASSO PCR outlier removed", ylim=c(0, 4.0),
       cex.axis=1.5, cex.main=1.5, cex.lab=1.5)
mtext("(b)",side=3,line=-1.9,adj=c(0.1),cex=2,col="black")
```

\begin{figure}
\centering\includegraphics[width=0.95\linewidth]{observer-sensitivity_files/figure-latex/loo_fort-1.pdf}
\caption{Plot of Fort data LOO Pareto shape estimates with (a) and without (b) outlying observation. Values less than 0.5 show good model performance and values over 1.0 show poor model performance.}
\label{fig:fortlooplots}
\end{figure}

## Reconstruction Results

```{r simTablesOutlier, echo=FALSE, message=FALSE, eval=TRUE}
loo_preds_outlier <- data.frame(looic= NA)
loo_ic_outlier <- data.frame(cbind(c(loo_pca_ssvs_outlier$looic,
                                          loo_pca_lasso_outlier$looic),
                                        c(loo_ppca_ssvs_outlier$looic,
                                          loo_ppca_lasso_outlier$looic),
                                        c(loo_tpca_ssvs_outlier$looic,
                                          loo_tpca_lasso_outlier$looic),
                                        c(loo_tppca_ssvs_outlier$looic,
                                          loo_tppca_lasso_outlier$looic)))
rownames(loo_ic_outlier) <- c("SSVS", "LASSO")
colnames(loo_ic_outlier) <- c("PCR", "pPCR", "PCR", "pPCR")
addtorow$pos <- list(0)
addtorow$command <- c("\\multicolumn{1}{|r|}{} & \\multicolumn{2}{c|}{Gaussian} & \\multicolumn{2}{c|}{Robust} \\\\
& PCR & pPCR & PCR & pPCR \\\\\n")
print(xtable(loo_ic_outlier, digits=0, align=c("|r|rr|rr|")),
      hline.after=c(-1, 0, nrow(loo_ic_fort)), add.to.row=addtorow,
      file="loooutlier.tex", floating=FALSE, size="small",
      include.colnames=FALSE)

fortscores <- data.frame(cbind(c(loo_pca_ssvs_fort$looic,
                                 loo_ppca_ssvs_fort$looic,
                                 loo_pca_lasso_fort$looic,
                                 loo_ppca_lasso_fort$looic),
                               c(loo_tpca_ssvs_fort$looic,
                                 loo_tppca_ssvs_fort$looic,
                                 loo_tpca_lasso_fort$looic,
                                 loo_tppca_lasso_fort$looic),
                               c(loo_pca_ssvs_outlier$looic,
                                 loo_ppca_ssvs_outlier$looic,
                                 loo_pca_lasso_outlier$looic,
                                 loo_ppca_lasso_outlier$looic),
                               c(loo_tpca_ssvs_outlier$looic,
                                 loo_tppca_ssvs_outlier$looic,
                                 loo_tpca_lasso_outlier$looic,
                                 loo_tppca_lasso_outlier$looic)))
rownames(fortscores) <- c("SSVS PCR", "SSVS pPCR", "LASSO PCR", "LASSO pPCR")
addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- c("\\multicolumn{1}{r}{} & \\multicolumn{2}{c}{Full Data} & \\multicolumn{2}{c}{Outlier Removed} \\\\
 Model & Gaussian & Robust & Gaussian & Robust \\\\\n")
print(xtable(fortscores, digits=0, align=c("rrrrr")),
      hline.after=c(-1, 0, nrow(fortscores)), add.to.row=addtorow,
      file="fort-scores.tex",
      floating=FALSE, size="small", include.colnames=FALSE)

```


## Plot reconstructed surfaces

```{r fortPlotstPCA, echo=TRUE, eval=TRUE, include=FALSE, fig.height=6, fig.width=12, eval=TRUE}
library(RColorBrewer)
fort.raster.tmp <- fort.raster
fort.raster.sd <- fort.raster
min.fort <- rep(NA, 4)
max.fort <- rep(NA, 4)
min.fort.sd <- rep(NA, 4)
max.fort.sd <- rep(NA, 4)

subset <- c(12, 28, 56, 71)
for(ii in 1:4){
  i <- subset[ii]
  values(fort.raster.tmp[[i]])[ - na.rows] <- y_tilde_mean_tpca_lasso_outlier[, i]
  values(fort.raster.sd[[i]])[ - na.rows] <- y_tilde_sd_tpca_lasso_outlier[, i]
  min.fort[ii] <- min(y_tilde_mean_tpca_lasso_outlier[, i],
                      y_tilde_mean_tppca_lasso_outlier[, i])
  max.fort[ii] <- max(y_tilde_mean_tpca_lasso_outlier[, i],
                      y_tilde_mean_tppca_lasso_outlier[, i])
  min.fort.sd[ii] <- min(y_tilde_sd_tpca_lasso_outlier[, i],
                         y_tilde_sd_tppca_lasso_outlier[, i])
  max.fort.sd[ii] <- max(y_tilde_sd_tpca_lasso_outlier[, i],
                         y_tilde_sd_tppca_lasso_outlier[, i])
}


layout(matrix(c(1, 1, 1, 2, 2, 2, 9, 3, 3, 3, 4, 4, 4, 10,
                1, 1, 1, 2, 2, 2, 9, 3, 3, 3, 4, 4, 4, 10,
                1, 1, 1, 2, 2, 2, 9, 3, 3, 3, 4, 4, 4, 10,
                5, 5, 5, 6, 6, 6, 9, 7, 7, 7, 8, 8, 8, 10,
                5, 5, 5, 6, 6, 6, 9, 7, 7, 7, 8, 8, 8, 10,
                5, 5, 5, 6, 6, 6, 9, 7, 7, 7, 8, 8, 8, 10), 6, 14, byrow=TRUE))
par(mar=c(3.5, 3.25, 1.75, 0.5) + 0.1, oma=c(0, 0, 0.25, 0.25) + 0.1)

for(i in subset[1:2]){
  if (i == subset[1]) {
    image(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(min(min.fort), max(max.fort)), ylab = '', xlab='', xaxt="n",
          cex.main=2.5)
    mtext("latitude", 2, line=2)
  } else {
    image(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(min(min.fort), max(max.fort)), ylab = '', xlab='', yaxt="n",
          xaxt="n", cex.main=2.5)
  }
  if(i == subset[1]){
    mtext("(a)",side=3,line=-1.9,adj=c(0.1),cex=2,col="black")
  }
  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, col = 'black', cex = 1.5)
}

for(i in subset[1:2]){
  image(fort.raster.sd[[i]], main = paste(i + 1819),
        col=colorRampPalette(brewer.pal(8, "YlGn"))(64),
        # cm.colors(64),
        zlim = c(min(min.fort.sd), max(max.fort.sd)),
        ylab = '', xlab='', yaxt="n", xaxt="n", cex.main=2.5)
  map(database = 'state', add = TRUE, col = 'black')
  if(i == subset[1]){
    mtext("(b)",side=3,line=-1.9,adj=c(0.1),cex=2,col="black")
  }
  points(prism.points[H.list[[i]], ], pch = 16, col = 'black', cex = 1.5)
}

for(i in subset[3:4]){
  if (i == subset[3]) {
    image(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(min(min.fort), max(max.fort)), ylab = '', xlab='',
          cex.main=2.5)
    mtext("latitude", 2, line=2)
    mtext("longitude", 1, line=2.25)
  } else {
    image(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(min(min.fort), max(max.fort)), ylab = '', xlab='', yaxt="n",
          cex.main=2.5)
  }

  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, col = 'black', cex = 1.5)
}

for(i in subset[3:4]){
  image(fort.raster.sd[[i]], main = paste(i + 1819),
        col=colorRampPalette(brewer.pal(8, "YlGn"))(64),
        # cm.colors(64),
        zlim = c(min(min.fort.sd), max(max.fort.sd)),
        ylab = '', xlab='', yaxt="n", cex.main=2.5)
  mtext("longitude", 1, line=2.25)
  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, col = 'black', cex = 1.5)
}
image.scale(fort.raster.tmp[[i]], #col = topo.colors(64),
            col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
            zlim = c(min(min.fort), max(max.fort)), horiz=FALSE)
image.scale(fort.raster.sd[[i]], col=colorRampPalette(brewer.pal(8, "YlGn"))(64),
            # cm.colors(64),
            zlim = c(min(min.fort.sd), max(max.fort.sd)), horiz=FALSE)
```

\begin{figure}
\centering\includegraphics[width=0.95\linewidth]{observer-sensitivity_files/figure-latex/fortPlotstPCA-1.pdf}
\caption{Reconstruction of temperature using robust PCR model for four representative years. Figures show posterior predictive mean (a) and standard deviation (b).}
\label{fig:tPCRFort}
\end{figure}

```{r fortPlotstpPCA, echo=TRUE, eval=TRUE, include=FALSE, fig.height=6, fig.width=12, eval=TRUE}
fort.raster.tmp <- fort.raster
fort.raster.sd <- fort.raster
min.fort <- rep(NA, 4)
max.fort <- rep(NA, 4)
min.fort.sd <- rep(NA, 4)
max.fort.sd <- rep(NA, 4)

subset <- c(12, 28, 56, 71)
for(ii in 1:4){
  i <- subset[ii]
  values(fort.raster.tmp[[i]])[ - na.rows] <- y_tilde_mean_tppca_lasso_outlier[, i]
  values(fort.raster.sd[[i]])[ - na.rows] <- y_tilde_sd_tppca_lasso_outlier[, i]
  min.fort[ii] <- min(y_tilde_mean_tpca_lasso_outlier[, i],
                      y_tilde_mean_tppca_lasso_outlier[, i])
  max.fort[ii] <- max(y_tilde_mean_tpca_lasso_outlier[, i],
                      y_tilde_mean_tppca_lasso_outlier[, i])
  min.fort.sd[ii] <- min(y_tilde_sd_tpca_lasso_outlier[, i],
                         y_tilde_sd_tppca_lasso_outlier[, i])
  max.fort.sd[ii] <- max(y_tilde_sd_tpca_lasso_outlier[, i],
                         y_tilde_sd_tppca_lasso_outlier[, i])
}


layout(matrix(c(1, 1, 1, 2, 2, 2, 9, 3, 3, 3, 4, 4, 4, 10,
                1, 1, 1, 2, 2, 2, 9, 3, 3, 3, 4, 4, 4, 10,
                1, 1, 1, 2, 2, 2, 9, 3, 3, 3, 4, 4, 4, 10,
                5, 5, 5, 6, 6, 6, 9, 7, 7, 7, 8, 8, 8, 10,
                5, 5, 5, 6, 6, 6, 9, 7, 7, 7, 8, 8, 8, 10,
                5, 5, 5, 6, 6, 6, 9, 7, 7, 7, 8, 8, 8, 10), 6, 14, byrow=TRUE))
par(mar=c(3.5, 3.25, 1.75, 0.5) + 0.1, oma=c(0, 0, 0.25, 0.25) + 0.1)

for(i in subset[1:2]){
  if (i == subset[1]) {
    image(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(min(min.fort), max(max.fort)), ylab = '', xlab='',
          yaxt="n", xaxt="n", cex.main=2.5)
    mtext("latitude", 2, line=2)
  } else {
    image(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(min(min.fort), max(max.fort)), ylab = '', xlab='',
          yaxt="n", xaxt="n", cex.main=2.5)
  }
  if(i == subset[1]){
    mtext("(a)",side=3,line=-1.9,adj=c(0.1),cex=2,col="black")
  }
  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, col = 'black', cex = 1.5)
}

for(i in subset[1:2]){
  image(fort.raster.sd[[i]], main = paste(i + 1819), #col = cm.colors(64),
        col=colorRampPalette(brewer.pal(8, "YlGn"))(64),
        zlim = c(min(min.fort.sd), max(max.fort.sd)),
        ylab = '', xlab='', yaxt="n", xaxt="n", cex.main=2.5)
  map(database = 'state', add = TRUE, col = 'black')
  if(i == subset[1]){
    mtext("(b)",side=3,line=-1.9,adj=c(0.1),cex=2,col="black")
  }
  points(prism.points[H.list[[i]], ], pch = 16, col = 'black', cex = 1.5)
}

for(i in subset[3:4]){
  if (i == subset[3]) {
    image(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(min(min.fort), max(max.fort)), ylab = '', xlab='',
          cex.main=2.5)
    mtext("latitude", 2, line=2)
    mtext("longitude", 1, line=2.25)
  } else {
    image(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(min(min.fort), max(max.fort)), ylab = '', xlab='', yaxt="n",
          cex.main=2.5)
    mtext("longitude", 1, line=2.25)
  }
  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, col = 'black', cex = 1.5)
}

for(i in subset[3:4]){
  image(fort.raster.sd[[i]], main = paste(i + 1819), #col = cm.colors(64),
        col=colorRampPalette(brewer.pal(8, "YlGn"))(64),
        zlim = c(min(min.fort.sd), max(max.fort.sd)),
        ylab = '', xlab='', yaxt="n", cex.main=2.5)
  mtext("longitude", 1, line=2.25)
  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, col = 'black', cex = 1.5)
}
image.scale(fort.raster.tmp[[i]], #col = topo.colors(64),
            col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
            zlim = c(min(min.fort), max(max.fort)), horiz=FALSE)
image.scale(fort.raster.sd[[i]], #col = cm.colors(64),
            col=colorRampPalette(brewer.pal(8, "YlGn"))(64),
            zlim = c(min(min.fort.sd), max(max.fort.sd)), horiz=FALSE)
```

\begin{figure}
\centering\includegraphics[width=0.95\linewidth]{observer-sensitivity_files/figure-latex/fortPlotstPCA-1.pdf}
\caption{Reconstruction of temperature using robust probabilistic PCR model for four representative years. Figures show posterior predictive mean (a) and standard deviation (b).}
\label{fig:tpPCRFort}
\end{figure}

## Plot difference in reconstructed surfaces

```{r fortPlotsDiff, echo=TRUE, eval=TRUE, include=FALSE, fig.height=6, fig.width=12, eval=TRUE}
library(RColorBrewer)
fort.raster.tmp <- fort.raster
fort.raster.sd <- fort.raster
min.fort <- rep(NA, 4)
max.fort <- rep(NA, 4)
min.fort.sd <- rep(NA, 4)
max.fort.sd <- rep(NA, 4)

subset <- c(12, 28, 56, 71)
for(ii in 1:4){
  i <- subset[ii]
  values(fort.raster.tmp[[i]])[ - na.rows] <- y_tilde_mean_tpca_lasso_outlier[, i] -
    y_tilde_mean_tppca_lasso_outlier[, i]
  values(fort.raster.sd[[i]])[ - na.rows] <- y_tilde_sd_tpca_lasso_outlier[, i] -
    y_tilde_sd_tppca_lasso_outlier[, i]
  min.fort[ii] <- min(y_tilde_mean_tpca_lasso_outlier[, i] -
                        y_tilde_mean_tppca_lasso_outlier[, i])
  max.fort[ii] <- max(y_tilde_mean_tpca_lasso_outlier[, i] -
                        y_tilde_mean_tppca_lasso_outlier[, i])
  min.fort.sd[ii] <- min(y_tilde_sd_tpca_lasso_outlier[, i] -
                           y_tilde_sd_tppca_lasso_outlier[, i])
  max.fort.sd[ii] <- max(y_tilde_sd_tpca_lasso_outlier[, i] -
                           y_tilde_sd_tppca_lasso_outlier[, i])
}


layout(matrix(c(1, 1, 1, 2, 2, 2, 9, 3, 3, 3, 4, 4, 4, 10,
                1, 1, 1, 2, 2, 2, 9, 3, 3, 3, 4, 4, 4, 10,
                1, 1, 1, 2, 2, 2, 9, 3, 3, 3, 4, 4, 4, 10,
                5, 5, 5, 6, 6, 6, 9, 7, 7, 7, 8, 8, 8, 10,
                5, 5, 5, 6, 6, 6, 9, 7, 7, 7, 8, 8, 8, 10,
                5, 5, 5, 6, 6, 6, 9, 7, 7, 7, 8, 8, 8, 10), 6, 14, byrow=TRUE))
par(mar=c(3.5, 3.25, 1.75, 0.5) + 0.1, oma=c(0, 0, 0.25, 0.25) + 0.1)

for(i in subset[1:2]){
  if (i == subset[1]) {
    image(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(- max(abs(min.fort), abs(max.fort)), max(abs(min.fort), abs(max.fort))),
          ylab = '', xlab='', xaxt="n", cex.main=2.5)
    mtext("latitude", 2, line=2)
  } else {
    image(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(- max(abs(min.fort), abs(max.fort)), max(abs(min.fort), abs(max.fort))),
          ylab = '', xlab='', yaxt="n", xaxt="n", cex.main=2.5)
  }
  if(i == subset[1]){
    mtext("(a)",side=3,line=-1.9,adj=c(0.1),cex=2,col="black")
  }
  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, col = 'black', cex = 1.5)
}

for(i in subset[1:2]){
  image(fort.raster.sd[[i]], main = paste(i + 1819),
        col=colorRampPalette(brewer.pal(8, "PuOr"))(64),
        # cm.colors(64),
        zlim = c(- max(abs(min.fort), abs(max.fort)), max(abs(min.fort), abs(max.fort))),
        ylab = '', xlab='', yaxt="n", xaxt="n", cex.main=2.5)
  map(database = 'state', add = TRUE, col = 'black')
  if(i == subset[1]){
    mtext("(b)",side=3,line=-1.9,adj=c(0.1),cex=2,col="black")
  }
  points(prism.points[H.list[[i]], ], pch = 16, col = 'black', cex = 1.5)
}

for(i in subset[3:4]){
  if (i == subset[3]) {
    image(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(- max(abs(min.fort), abs(max.fort)), max(abs(min.fort), abs(max.fort))),
          ylab = '', xlab='', cex.main=2.5)
    mtext("latitude", 2, line=2)
    mtext("longitude", 1, line=2.25)
  } else {
    image(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
          col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
          zlim = c(- max(abs(min.fort), abs(max.fort)), max(abs(min.fort), abs(max.fort))),
          ylab = '', xlab='', yaxt="n", cex.main=2.5)
  }

  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, col = 'black', cex = 1.5)
}

for(i in subset[3:4]){
  image(fort.raster.sd[[i]], main = paste(i + 1819),
        col=colorRampPalette(brewer.pal(8, "PuOr"))(64),
        # cm.colors(64),
        zlim = c(- max(abs(min.fort), abs(max.fort)), max(abs(min.fort), abs(max.fort))),
        ylab = '', xlab='', yaxt="n", cex.main=2.5)
  mtext("longitude", 1, line=2.25)
  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, col = 'black', cex = 1.5)
}
image.scale(fort.raster.tmp[[i]], #col = topo.colors(64),
            col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
            zlim = c(- max(abs(min.fort), abs(max.fort)), max(abs(min.fort), abs(max.fort))),
            horiz=FALSE)
image.scale(fort.raster.sd[[i]], col=colorRampPalette(brewer.pal(8, "PuOr"))(64),
            # cm.colors(64),
            zlim = c(- max(abs(min.fort), abs(max.fort)), max(abs(min.fort), abs(max.fort))),
            horiz=FALSE)
```

\begin{figure}
\centering\includegraphics[width=0.95\linewidth]{observer-sensitivity_files/figure-latex/fortPlotsDiff-1.pdf}
\caption{Difference in reconstruction of temperature using robust PCR minus robust probabilistic PCR model for four representative years. Figures show posterior predictive difference in mean (a) and difference in standard deviation (b).}
\label{fig:tpPCRFort}
\end{figure}

## Plot time series reconstructions at four arbitrary locations

```{r TimeSeries, echo=TRUE, eval=TRUE, include=FALSE, fig.height=6, fig.width=12, eval=TRUE}
library(RColorBrewer)
fort.raster.tmp <- fort.raster
fort.raster.sd <- fort.raster
madison_longlat <- c(-89.41012, 43.0731)
minneapolis_longlat <- c(-93.2650, 44.9778)
detroit_longlat <- c(-83.0458, 42.3314)
champaign_longlat <- c(-88.2073, 40.1106)


madison_idx <- which(pointDistance(prism.points, madison_longlat, lonlat=TRUE) ==
                       min(pointDistance(prism.points, madison_longlat, lonlat=TRUE)))
minneapolis_idx <- which(pointDistance(prism.points, minneapolis_longlat, lonlat=TRUE) ==
                           min(pointDistance(prism.points, minneapolis_longlat, lonlat=TRUE)))
detroit_idx <- which(pointDistance(prism.points, detroit_longlat, lonlat=TRUE) ==
                       min(pointDistance(prism.points, detroit_longlat, lonlat=TRUE)))
champaign_idx <- which(pointDistance(prism.points, champaign_longlat, lonlat=TRUE) ==
                         min(pointDistance(prism.points, champaign_longlat, lonlat=TRUE)))

madison_temp <- matrix(0, 75 + 116, 2)
minneapolis_temp <- matrix(0, 75 + 116, 2)
detroit_temp <- matrix(0, 75 + 116, 2)
champaign_temp <- matrix(0, 75 + 116, 2)
madison_temp_sd <- matrix(0, 75 + 116, 2)
minneapolis_temp_sd <- matrix(0, 75 + 116, 2)
detroit_temp_sd <- matrix(0, 75 + 116, 2)
champaign_temp_sd <- matrix(0, 75 + 116, 2)

for (i in 1:(75 + 116)) {
  if (i <= 75) {
    madison_temp[i, ] <- c(y_tilde_mean_tpca_lasso_outlier[madison_idx, i],
                           y_tilde_mean_tppca_lasso_outlier[madison_idx, i])
    minneapolis_temp[i, ] <- c(y_tilde_mean_tpca_lasso_outlier[minneapolis_idx, i],
                               y_tilde_mean_tppca_lasso_outlier[minneapolis_idx, i])
    detroit_temp[i, ] <- c(y_tilde_mean_tpca_lasso_outlier[detroit_idx, i],
                           y_tilde_mean_tppca_lasso_outlier[detroit_idx, i])
    champaign_temp[i, ] <- c(y_tilde_mean_tpca_lasso_outlier[champaign_idx, i],
                             y_tilde_mean_tppca_lasso_outlier[champaign_idx, i])
    madison_temp_sd[i, ] <- c(y_tilde_sd_tpca_lasso_outlier[madison_idx, i],
                              y_tilde_sd_tppca_lasso_outlier[madison_idx, i])
    minneapolis_temp_sd[i, ] <- c(y_tilde_sd_tpca_lasso_outlier[minneapolis_idx, i],
                                  y_tilde_sd_tppca_lasso_outlier[minneapolis_idx, i])
    detroit_temp_sd[i, ] <- c(y_tilde_sd_tpca_lasso_outlier[detroit_idx, i],
                              y_tilde_sd_tppca_lasso_outlier[detroit_idx, i])
    champaign_temp_sd[i, ] <- c(y_tilde_sd_tpca_lasso_outlier[champaign_idx, i],
                                y_tilde_sd_tppca_lasso_outlier[champaign_idx, i])
  } else {
    ii <- i - 75
    madison_temp[i, ] <- X.farenheit[madison_idx, ii]
    minneapolis_temp[i, ] <- X.farenheit[madison_idx, ii]
    detroit_temp[i, ] <- X.farenheit[madison_idx, ii]
    champaign_temp[i, ] <- X.farenheit[madison_idx, ii]
  }
}

## adjust temp from 8:30AM to temperatures over a one minute grid choosing
## (T_min+T_max)/2 resulting in constant mean shift of 3.027014

madison_temp_adjusted <- madison_temp
minneapolis_temp_adjusted <- minneapolis_temp
detroit_temp_adjusted <- detroit_temp
champaign_temp_adjusted <- champaign_temp
madison_temp_adjusted[1:75, ] <- madison_temp_adjusted[1:75, ]+3.027014
minneapolis_temp_adjusted[1:75, ] <- minneapolis_temp_adjusted[1:75, ]+3.027014
detroit_temp_adjusted[1:75, ] <- detroit_temp_adjusted[1:75, ]+3.027014
champaign_temp_adjusted[1:75, ] <- champaign_temp_adjusted[1:75, ]+3.027014

require(ggplot2)
time.dat <- data.frame(Temperature = c(madison_temp_adjusted[1:75, ],
                                       minneapolis_temp_adjusted[1:75, ],
                                       detroit_temp_adjusted[1:75, ],
                                       champaign_temp_adjusted[1:75, ]),
                       temp_min = c(madison_temp_adjusted[1:75, ] -
                                      2*madison_temp_sd[1:75, ],
                                    minneapolis_temp_adjusted[1:75, ] -
                                      2*minneapolis_temp_sd[1:75, ],
                                    detroit_temp_adjusted[1:75, ] -
                                      2*detroit_temp_sd[1:75, ],
                                    champaign_temp_adjusted[1:75, ] -
                                      2*champaign_temp_sd[1:75, ]),
                       temp_max = c(madison_temp_adjusted[1:75, ] +
                                      2*madison_temp_sd[1:75, ],
                                    minneapolis_temp_adjusted[1:75, ] +
                                      2*minneapolis_temp_sd[1:75, ],
                                    detroit_temp_adjusted[1:75, ] +
                                      2*detroit_temp_sd[1:75, ],
                                    champaign_temp_adjusted[1:75, ] +
                                      2*champaign_temp_sd[1:75, ]),
                       sd = c(madison_temp_sd[1:75, ], minneapolis_temp_sd[1:75, ],
                              detroit_temp_sd[1:75, ], champaign_temp_sd[1:75, ]),
                       location = c(rep("Madison", 150), rep("Minneapolis", 150),
                                    rep("Detroit", 150), rep("Champaign", 150)),
                       model = rep(c(rep("tPCA LASSO", 75), rep("tpPCA LASSO", 75)), times=4),
                       Year = rep(1820:1894, 8))
# time.dat <- data.frame(Temperature = c(madison_temp[1:75, ], minneapolis_temp[1:75, ],
#                                        detroit_temp[1:75, ], champaign_temp[1:75, ]),
#                        temp_min = c(madison_temp[1:75, ]-2*madison_temp_sd[1:75, ],
#                                     minneapolis_temp[1:75, ]-2*minneapolis_temp_sd[1:75, ],
#                                     detroit_temp[1:75, ]-2*detroit_temp_sd[1:75, ],
#                                     champaign_temp[1:75, ]-2*champaign_temp_sd[1:75, ]),
#                        temp_max = c(madison_temp[1:75, ]+2*madison_temp_sd[1:75, ],
#                                     minneapolis_temp[1:75, ]+2*minneapolis_temp_sd[1:75, ],
#                                     detroit_temp[1:75, ]+2*detroit_temp_sd[1:75, ],
#                                     champaign_temp[1:75, ]+2*champaign_temp_sd[1:75, ]),
#                        sd = c(madison_temp_sd[1:75, ], minneapolis_temp_sd[1:75, ],
#                               detroit_temp_sd[1:75, ], champaign_temp_sd[1:75, ]),
#                        location = c(rep("Madison", 150), rep("Minneapolis", 150),
#                                     rep("Detroit", 150), rep("Champaign", 150)),
#                        model = rep(c(rep("tPCA LASSO", 75), rep("tpPCA LASSO", 75)), times=4),
#                        Year = rep(1820:1894, 8))

ts1 <- ggplot(time.dat, aes(x=Year, y=Temperature, group=model, color=model)) +
  geom_line(size=1.25, alpha=0.8) + facet_grid(location ~ .) +
  #scale_fill_brewer(palette="RdBu") +
  geom_ribbon(aes(ymin=temp_min, ymax=temp_max, fill=model), alpha=0.3) +
  theme(text=element_text(size=10)) #+
# geom_vline(xintercept=1846) + geom_vline(xintercept=1848)s
library(gridExtra)
# grid.arrange(ts1, top=textGrob("(a)"))
ts1


library(cowplot)
obs.dat <- data.frame(Year=1820:1894, N=nobs_outlier)
ts2 <- ggplot(obs.dat, aes(x=Year, y=N)) + geom_bar(stat="identity") +
  labs(y=expression(m[t])) + theme(text=element_text(size=10)) +
  scale_y_continuous(breaks=c(50, 150, 250)) #+
# geom_vline(xintercept=1846) +
# geom_vline(xintercept=1848)
# grid.arrange(ts2, top=textGrob("(b)"))
ts2
# ggdraw() +
#   draw_plot(ts1, 0.01, 0.2, 1, .8) +
#   draw_plot(ts2, 0, 0, 0.85, 0.2)
ggdraw() +
  draw_plot(grid.arrange(ts1, top=textGrob("(a)")), 0.01, 0.2, 1, .8) +
  draw_plot(grid.arrange(ts2, top=textGrob("(b)")), 0, 0, 0.85, 0.2)
# plot_grid(ts1, ts2, ncol=1, align="h")


library(ggplot2)
library(grid)
library(gridExtra)
library(gtable)

gtable_frame <- function(g, width=unit(1,"null"), height=unit(1,"null")){
  panels <- g[["layout"]][grepl("panel", g[["layout"]][["name"]]), ]
  ll <- unique(panels$l)
  tt <- unique(panels$t)

  fixed_ar <- g$respect
  if(fixed_ar) { # there lies madness, want to align despite aspect ratio constraints
    ar <- as.numeric(g$heights[tt[1]]) / as.numeric(g$widths[ll[1]])
    height <- width * ar
    g$respect <- FALSE
  }

  core <- g[seq(min(tt), max(tt)), seq(min(ll), max(ll))]
  top <- g[seq(1, min(tt)-1), ]
  bottom <- g[seq(max(tt)+1, nrow(g)), ]
  left <- g[, seq(1, min(ll)-1)]
  right <- g[, seq(max(ll)+1, ncol(g))]

  fg <- nullGrob()
  lg <-  if(length(left))  g[seq(min(tt), max(tt)), seq(1, min(ll)-1)] else fg
  rg <- if(length(right)) g[seq(min(tt), max(tt)), seq(max(ll)+1,ncol(g))] else fg
  grobs = list(fg, g[seq(1, min(tt)-1), seq(min(ll), max(ll))], fg,
               lg, g[seq(min(tt), max(tt)), seq(min(ll), max(ll))], rg,
               fg, g[seq(max(tt)+1, nrow(g)), seq(min(ll), max(ll))], fg)
  widths <- unit.c(sum(left$widths), width, sum(right$widths))
  heights <- unit.c(sum(top$heights), height, sum(bottom$heights))
  all <- gtable_matrix("all", grobs = matrix(grobs, ncol=3, nrow=3, byrow = TRUE),
                       widths = widths, heights = heights)
  all[["layout"]][5,"name"] <- "panel" # make sure knows where the panel is for nested calls
  if(fixed_ar)  all$respect <- TRUE
  all
}


p1 <- ggplotGrob(ts1)
p2 <- ggplotGrob(ts2)
fp1 <- gtable_frame(p1, height=unit(4, "null"))
fp2 <- gtable_frame(p2)
grid.newpage()
grid.draw(rbind(fp1, fp2))


layout(matrix(1:4, 2, 2))
matplot(madison_temp[1:75, ], type='l', lty=1)
for(i in 1:2) {
  lines(madison_temp[1:75, i] + 2*madison_temp_sd[1:75, i], type= 'l', lty=2, col=i)
  lines(madison_temp[1:75, i] - 2*madison_temp_sd[1:75, i], type= 'l', lty=2, col=i)
}
# abline(v=76)
matplot(minneapolis_temp[1:75, ], type='l', lty=1)
for(i in 1:2) {
  lines(minneapolis_temp[1:75, i] + 2*minneapolis_temp_sd[1:75, i], type= 'l', lty=2, col=i)
  lines(minneapolis_temp[1:75, i] - 2*minneapolis_temp_sd[1:75, i], type= 'l', lty=2, col=i)
}
# abline(v=76)
matplot(detroit_temp[1:75, ], type='l', lty=1)
for(i in 1:2) {
  lines(detroit_temp[1:75, i] + 2*detroit_temp_sd[1:75, i], type= 'l', lty=2, col=i)
  lines(detroit_temp[1:75, i] - 2*detroit_temp_sd[1:75, i], type= 'l', lty=2, col=i)
}
# abline(v=76)
matplot(champaign_temp[1:75, ], type='l', lty=1)
for(i in 1:2) {
  lines(champaign_temp[1:75, i] + 2*champaign_temp_sd[1:75, i], type= 'l', lty=2, col=i)
  lines(champaign_temp[1:75, i] - 2*champaign_temp_sd[1:75, i], type= 'l', lty=2, col=i)
}
# abline(v=76)


layout(matrix(1:4, 2, 2))
matplot(madison_temp, type='l', lty=1)
abline(v=76)
matplot(minneapolis_temp, type='l', lty=1)
abline(v=76)
matplot(detroit_temp, type='l', lty=1)
abline(v=76)
matplot(champaign_temp, type='l', lty=1)
abline(v=76)


layout(matrix(1:4, 2, 2))
matplot(madison_temp_adjusted, type='l', lty=1)
abline(v=76)
matplot(minneapolis_temp_adjusted, type='l', lty=1)
abline(v=76)
matplot(detroit_temp_adjusted, type='l', lty=1)
abline(v=76)
matplot(champaign_temp_adjusted, type='l', lty=1)
abline(v=76)

```

\begin{figure}
\centering\includegraphics[width=0.95\linewidth]{observer-sensitivity_files/figure-latex/TimeSeries-5.pdf}
\caption{Posterior predictions of mean July temperature with associated 95\% credible intervals at Champaign, Illinois, Detriot, Michigan, Madison, Wisconsin, and Minneapolis, Minnesota (a). The temporal change in the number of observations is shown in (b). Notice that the uncertainties in the point reconstructions are smallest in years with larger samples and largest in years with few samples. In general, the robust pPCR credible intervals are larger than the robust PCR credible intervals.}
\label{fig:time}
\end{figure}


## Full time series of plots in appendix

```{r fortPlotstPCAFull, echo=FALSE, include=TRUE, fig.height=6, fig.width=6, fig.keep='all', fig.show='hide', cache=TRUE, eval=TRUE}
fort.raster.tmp <- fort.raster
fort.raster.sd <- fort.raster
min.fort <- rep(NA, t)
max.fort <- rep(NA, t)
min.fort.sd <- rep(NA, t)
max.fort.sd <- rep(NA, t)

for(i in 1:t){
  values(fort.raster.tmp[[i]])[ - na.rows] <- y_tilde_mean_tpca_lasso_outlier[, i]
  values(fort.raster.sd[[i]])[ - na.rows] <- y_tilde_sd_tpca_lasso_outlier[, i]
  min.fort[i] <- min(y_tilde_mean_tpca_lasso_outlier,
                     y_tilde_mean_tppca_lasso_outlier)
  max.fort[i] <- max(y_tilde_mean_tpca_lasso_outlier,
                     y_tilde_mean_tppca_lasso_outlier)
  min.fort.sd[i] <- min(y_tilde_sd_tpca_lasso_outlier,
                        y_tilde_sd_tppca_lasso_outlier)
  max.fort.sd[i] <- max(y_tilde_sd_tpca_lasso_outlier,
                        y_tilde_sd_tppca_lasso_outlier)
}

par(mfrow=c(4, 4), mar=c(3, 3, 1.75, 1.5) + 0.1, oma=c(0, 0, 0.25, 0.5) + 0.1, mgp=c(2, 1, 0))
# par(mfrow=c(4, 4), mar=c(3, 3, 1, 1) + 0.1, oma=c(0, 0, 0, 0) + 0.1, mgp=c(2, 1, 0))
for(i in 1:t) {
  image.plot(fort.raster.tmp[[i]], main = paste(i + 1819),
             col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
             zlim = c(min(min.fort), max(max.fort)),
             ylab = 'latitude', xlab='longitude', cex.main=1.5)
  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, cex = 1)
}

par(mfrow=c(4, 4), mar=c(3, 3, 1.75, 1.5) + 0.1, oma=c(0, 0, 0.25, 0.5) + 0.1, mgp=c(2, 1, 0))
# par(mfrow=c(4, 4), mar=c(3, 3, 1, 1) + 0.1, oma=c(0, 0, 0, 0) + 0.1, mgp=c(2, 1, 0))
for(i in 1:t){
  image.plot(fort.raster.sd[[i]], main = paste(i + 1819), #col = cm.colors(64),
             col=colorRampPalette(brewer.pal(8, "YlGn"))(64),
             zlim = c(min(min.fort.sd), max(max.fort.sd)),
             ylab = 'latitude', xlab='longitude', cex.main=1.5)
  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, cex = 1, col = 'black')
}
```

```{r fortPlotstpPCAFull, echo=FALSE, include=TRUE, fig.height=6, fig.width=6, fig.keep='all', fig.show='hide', cache=TRUE, eval=TRUE}
fort.raster.tmp <- fort.raster
fort.raster.sd <- fort.raster
min.fort <- rep(NA, t)
max.fort <- rep(NA, t)
min.fort.sd <- rep(NA, t)
max.fort.sd <- rep(NA, t)

for(i in 1:t){
  values(fort.raster.tmp[[i]])[ - na.rows] <- y_tilde_mean_tpca_lasso_outlier[, i]
  values(fort.raster.sd[[i]])[ - na.rows] <- y_tilde_sd_tpca_lasso_outlier[, i]
  min.fort[i] <- min(y_tilde_mean_tpca_lasso_outlier,
                     y_tilde_mean_tppca_lasso_outlier)
  max.fort[i] <- max(y_tilde_mean_tpca_lasso_outlier,
                     y_tilde_mean_tppca_lasso_outlier)
  min.fort.sd[i] <- min(y_tilde_sd_tpca_lasso_outlier,
                        y_tilde_sd_tppca_lasso_outlier)
  max.fort.sd[i] <- max(y_tilde_sd_tpca_lasso_outlier,
                        y_tilde_sd_tppca_lasso_outlier)
}
par(mfrow=c(4, 4), mar=c(3, 3, 1.75, 1.5) + 0.1, oma=c(0, 0, 0.25, 0.5) + 0.1, mgp=c(2, 1, 0))
# par(mfrow=c(4, 4), mar=c(3, 3, 1, 1) + 0.1, oma=c(0, 0, 0, 0) + 0.1, mgp=c(2, 1, 0))
for(i in 1:t) {
  image.plot(fort.raster.tmp[[i]], main = paste(i + 1819), #col = topo.colors(64),
             col=rev(colorRampPalette(brewer.pal(8, "RdBu"))(64)),
             zlim = c(min(min.fort), max(max.fort)),
             ylab = 'latitude', xlab='longitude', cex.main=1.5)
  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, cex = 1)
}

par(mfrow=c(4, 4), mar=c(3, 3, 1.75, 1.5) + 0.1, oma=c(0, 0, 0.25, 0.5) + 0.1, mgp=c(2, 1, 0))
for(i in 1:t){
  image.plot(fort.raster.sd[[i]], main = paste(i + 1819), #col = cm.colors(64),
             col=colorRampPalette(brewer.pal(8, "YlGn"))(64),
             zlim = c(min(min.fort.sd), max(max.fort.sd)),
             ylab = 'latitude', xlab='longitude', cex.main=1.5)
  map(database = 'state', add = TRUE, col = 'black')
  points(prism.points[H.list[[i]], ], pch = 16, cex = 1, col = 'black')
}
```

